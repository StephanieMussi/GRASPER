{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 1\n",
      "XCon: Learning with Experts for\n",
      "Fine-grained Category Discovery\n",
      "Yixin Fei1\n",
      "yixin.feiyx@gmail.com\n",
      "Zhongkai Zhao1\n",
      "zhongkai.zhaok@gmail.com\n",
      "Siwei Yang1,3\n",
      "swyang.ac@gmail.com\n",
      "Bingchen Zhao2,3\n",
      "zhaobc.gm@gmail.com\n",
      "1 Tongji University\n",
      "Shanghai, China\n",
      "2 University of Edinburgh,\n",
      "Edinburgh, UK\n",
      "3 LunarAI\n",
      "Abstract\n",
      "We address the problem of generalized category discovery (GCD) in this paper, i.e.\n",
      "clustering the unlabeled images leveraging the information from a set of seen classes,\n",
      "where the unlabeled images could contain both seen classes and unseen classes. The seen\n",
      "classes can be seen as an implicit criterion of classes, which makes this setting different\n",
      "from unsupervised clustering where the cluster criteria may be ambiguous. We mainly\n",
      "concern the problem of discovering categories within a ﬁne-grained dataset since it is one\n",
      "of the most direct applications of category discovery, i.e. helping experts discover novel\n",
      "concepts within an unlabeled dataset using the implicit criterion set forth by the seen\n",
      "classes. State-of-the-art methods for generalized category discovery leverage contrastive\n",
      "learning to learn the representations, but the large inter-class similarity and intra-class\n",
      "variance pose a challenge for the methods because the negative examples may contain\n",
      "irrelevant cues for recognizing a category so the algorithms may converge to a local-\n",
      "minima. We present a novel method called Expert-Contrastive Learning (XCon) to help\n",
      "the model to mine useful information from the images by ﬁrst partitioning the dataset into\n",
      "sub-datasets using k-means clustering and then performing contrastive learning on each\n",
      "of the sub-datasets to learn ﬁne-grained discriminative features. Experiments on ﬁne-\n",
      "grained datasets show a clear improved performance over the previous best methods,\n",
      "indicating the effectiveness of our method.\n",
      "1\n",
      "Introduction\n",
      "Deep learning models have achieved super-human performance on many computer vi-\n",
      "sion problems where large-scale human annotations are available, such as image recogni-\n",
      "tion [5] and object detection [23]. However, collecting a dataset at scales like ImageNet or\n",
      "COCO is not always possible. Consider the scenario of ﬁne-grained recognition such as bird\n",
      "species recognition or medical image analysis, where the annotations require expert knowl-\n",
      "edge which could be costly to collect, also it is difﬁcult for the collected annotations to cover\n",
      "all the possible classes because new classes keep growing over time.\n",
      "The problem of generalized category discovery was recently formalized in [26], where\n",
      "the aim is to discover categories within the unlabeled data by leveraging the information\n",
      "© 2022. The copyright of this document resides with its authors.\n",
      "It may be distributed unchanged freely in print or electronic forms.\n",
      "arXiv:2208.01898v1  [cs.CV]  3 Aug 2022\n",
      "2 Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "DINO w/o our fine-tuning\n",
      "CUB\n",
      "DINO w/ our fine-tuning\n",
      "Figure 1: k-means results on the DINO features and features after ﬁne-tuning with our\n",
      "method. The images in each row represent a cluster in k-means. The clusters formed by\n",
      "DINO features are mainly based on the class irrelevant cues, e.g., background and object\n",
      "pose. The features learned by our method could cluster the images based on the correct cues,\n",
      "the object classes.\n",
      "from a set of labeled data. It is assumed that the labeled data contains similar yet distinct\n",
      "classes from the unlabeled data. The labeled data collected by human experts can be seen\n",
      "as an implicit criterion of classes which can be learned by the model to perform clustering\n",
      "on the unlabeled data. This setting is much harder than semi-supervised learning because\n",
      "generalized category discovery does not assume we know all the classes in the data while\n",
      "in semi-supervised learning the assumption is that the labeled data covers all the classes\n",
      "including ones in unlabeled data.\n",
      "In this paper, we speciﬁcally focus on ﬁne-grained generalized category discovery which\n",
      "is a more difﬁcult and practical problem than generic category discovery since ﬁeld experts\n",
      "are interested in the ﬁne-grained concepts in real applications, and they often have a labeled\n",
      "dataset representing the existing knowledge, so such a ﬁne-grained generalized category\n",
      "discovery method could help them make sense of the unlabeled set by clustering the unla-\n",
      "beled instance according to the criteria implicitly deﬁned in the labeled data. In ﬁne-grained\n",
      "category discovery, the main challenge is the large inter-class similarity and the intra-class\n",
      "variance, different classes may require the model to learn more discriminative features to\n",
      "be able to distinguish, e.g., two different birds could only differ in the beak. We have ob-\n",
      "served that an unsupervised representation (e.g. DINO) could cluster the data based on class\n",
      "irrelevant cues such as the object pose or the background, see the left part of Fig. 1. Based\n",
      "on this observation, we proposed a simple yet effective method to boost the performance\n",
      "of generalized category discovery on ﬁne-grained data named Expert Contrastive Learning\n",
      "(XCon).\n",
      "In our proposed XCon method, we partition the data into k expert sub-datasets by directly\n",
      "performing k-means clustering on self-supervised representations. These k sub-datasets can\n",
      "be used as a strong prior for the next learning phase because within each of the k sub-\n",
      "datasets, class-irrelevant cues will be so similar that the model will be forced to learn more\n",
      "class-relevant features within each sub-dataset. Each of these sub-datasets can be viewed\n",
      "as an expert dataset used to eliminate the negative inﬂuence introduced by certain kinds of\n",
      "class-irrelevant cues. To learn a robust representation from these datasets, we directly lever-\n",
      "age supervised contrastive learning [16] on the labeled data and unsupervised contrastive\n",
      "learning [2] on all the data.\n",
      "Our contribution is three-fold:\n",
      "• We observed that self-supervised representations can group the data based on class\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 3\n",
      "irrelevant cues which can be exploited to design further methods.\n",
      "• We proposed a method that can learn discriminative features for ﬁne-grained category\n",
      "discovery by partitioning the data into k sub-datasets.\n",
      "• We validated the effectiveness of our proposed method by setting a new state-of-the-art\n",
      "performance on seven tested generalized category discovery benchmarks.\n",
      "Our code is available at https://github.com/YiXXin/XCon.\n",
      "2\n",
      "Related Works\n",
      "2.1\n",
      "Novel Category Discovery\n",
      "Novel Category Discovery (NCD) aims to discover new object categories by transferring\n",
      "the knowledge learned from a set of relevant but different seen classes. This task was ﬁrst\n",
      "formalized in DTC [8], with earlier works [13, 14] tackling a similar problem. KCL [13]\n",
      "and MCL [14] utilize the pairwise similarity to transfer the clustering model to cross-task\n",
      "scenarios, which can be used to categorize the unseen classes further. A common three-\n",
      "step learning pipeline is proposed in RankStat [9] where the representation is ﬁrst learned\n",
      "with self-supervision on all the data and then ﬁne-tuned on the labeled data, the ﬁnal repre-\n",
      "sentation used for discovering novel categories is then further ﬁne-tuned using a pair-wise\n",
      "clustering loss on the unlabeled data. Since then, many works [4, 7, 9, 29, 30, 31] begin\n",
      "to focus on this NCD problem and present promising results. Contrastive learning has been\n",
      "explored under this NCD problem by NCL [30], showing strong performance.\n",
      "Efforts have also been made in extending this problem to the more challenging ﬁne-\n",
      "grained classiﬁcation scenario by DualRank [29], which leverages the local object parts in-\n",
      "formation to enhance the representations used for discovering novel categories. Our work\n",
      "also focuses on the challenging ﬁne-grained classiﬁcation scenario. The key difference with\n",
      "prior works is that we use k-means grouping on a self-supervised feature to provide infor-\n",
      "mative pairs for contrastive learning instead of using MixUp [30] or local object parts [29].\n",
      "Our work also builds on a newly proposed setting named Generalized Category Discov-\n",
      "ery (GCD) [26] where the unlabeled examples can come from both seen and unseen classes,\n",
      "which is a more realistic scenario than NCD.\n",
      "2.2\n",
      "Contrastive Learning\n",
      "Contrastive learning has been showing to be effective for learning representations [2, 11]\n",
      "in a self-supervised manner using the instance discrimination pretext [28] as the learning\n",
      "objective. Instance discrimination learns the representation by pushing negative examples\n",
      "away from each other and pulling positive examples closer in the embedding space. As\n",
      "informative examples are important for learning representations with contrastive learning,\n",
      "there are works following this direction trying to create more informative negative or positive\n",
      "pairs using MixUp [15, 32] or special augmentations [24].\n",
      "Our focus is to learn representations that can be used to discover novel ﬁne-grained\n",
      "categories within the unlabeled dataset, for which a strong representation is needed. By\n",
      "creating informative contrastive pairs by partitioning the dataset into k sub-datasets using\n",
      "k-means, examples within each sub-dataset will be similar so that the model will be forced\n",
      "to learn more discriminative features. Compared to previous GCD methods with contrastive\n",
      "learning [26], our method shows clear performance improvements.\n",
      "4 Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "Training Data\n",
      "…\n",
      "…\n",
      "𝐾Groups\n",
      "ℒcg\n",
      "Projection Heads\n",
      "ℒfg\n",
      "ViT-B\n",
      "…\n",
      "𝒟0\n",
      "Coarse-grained Set\n",
      "Fine-grained Set\n",
      "ViT-B\n",
      "ℎ0\n",
      "𝒟1\n",
      "𝒟2\n",
      "𝒟K\n",
      "ℎ1\n",
      "ℎ2\n",
      "ℎ𝐾\n",
      "Figure 2: Overview of our XCon framework. We ﬁrst partition the dataset into K sub-\n",
      "datasets using k-means clustering the DINO [1] pretrained representations, then we perform\n",
      "joint contrastive representation learning on each of the partitioned sub-datasets D1 ...DK as\n",
      "well as on the full dataset D0. Each of the partitioned sub-datasets will force the model to\n",
      "learn ﬁne-grained discriminative information, because the background is similar within each\n",
      "of the sub-datasets so the model will need to learn the difference on the objects to be able to\n",
      "distinguish the examples.\n",
      "3\n",
      "Methods\n",
      "In GCD, the training dataset contains two parts, a labeled dataset Dl =\n",
      "\b\n",
      "(xl\n",
      "i,yl\n",
      "i)\n",
      "\t\n",
      "and\n",
      "an unlabeled dataset Du = {(xu\n",
      "i ,yu\n",
      "i )}, where yl\n",
      "i ∈Cl and yu\n",
      "i ∈Cu. Cl are only composed of\n",
      "seen classes while Cu are composed of both seen and unseen classes, thus Cl ⊆Cu. The goal\n",
      "of GCD is to learn a model to categorize the instances in Du by leveraging the information\n",
      "from Dl. Compared to the previous NCD problem that considers the class sets as Cl ∩Cu = /\n",
      "0,\n",
      "GCD is more challenging and practical.\n",
      "It has been shown that self-supervised ViT features [1] could be a good initialization\n",
      "for representation learning in GCD [26]. In Vaze et al. [26], contrastive learning is used to\n",
      "ﬁne-tune the representation using the information from both labeled and unlabeled datasets,\n",
      "and it is shown that contrastive learning could indeed improve the performance of the repre-\n",
      "sentation on the task of GCD. Informative contrastive pairs are important for representation\n",
      "learning, especially in the ﬁne-grained classiﬁcation setting where the model needs to learn\n",
      "subtle discriminative cues between categories. We proposed a simple method that partitions\n",
      "the dataset into k sub-datasets by using k-means on self-supervised features to help construct\n",
      "informative contrastive pairs for representations, the overview of our framework is shown\n",
      "in Fig. 2.\n",
      "3.1\n",
      "Preliminary\n",
      "In this section, we brieﬂy review the method proposed in Vaze et al. [26] for GCD,\n",
      "which consists of two parts, representation learning and class assignment. For representation\n",
      "learning, Vaze et al. [26] ﬁne-tunes the representation by performing supervised contrastive\n",
      "learning on the labeled data and unsupervised contrastive learning on all the data to avoid\n",
      "outﬁtting the seen classes.\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 5\n",
      "The unsupervised contrastive loss is deﬁned as\n",
      "Lu\n",
      "i = −log\n",
      "exp(zi · ˆ\n",
      "zi/τ)\n",
      "∑n 1[n̸=i] exp(zi ·zn/τ)\n",
      "(1)\n",
      "where zi = h(f(xi)) is the feature extracted by a backbone f(·) on the input image xi and\n",
      "projected to the embedding space via a projection head h(·), ˆ\n",
      "zi is the feature from another\n",
      "view of the input image ˆ\n",
      "xi.\n",
      "The supervised contrastive loss is deﬁned as\n",
      "Ls\n",
      "i = −\n",
      "1\n",
      "|N(i)| ∑\n",
      "q∈N(i)\n",
      "log\n",
      "exp(zi ·zq/τ)\n",
      "∑n 1[n̸=i] exp(zi ·zn/τ)\n",
      "(2)\n",
      "where N(i) is the set of indices of images in the minibatch that have the same label yi with\n",
      "the anchor image i.\n",
      "The ﬁnal learning objective is the combination of these two losses\n",
      "Lcoarse = (1−λ)\n",
      "∑\n",
      "i∈BU ∪BL\n",
      "Lu\n",
      "i +λ ∑\n",
      "i∈BL\n",
      "Ls\n",
      "i\n",
      "(3)\n",
      "where λ is used to balance between these two terms, BU is a minibatch of unlabeled images,\n",
      "and BL is a minibatch of labeled images.\n",
      "For class assignments, the semi-supervised k-means method is proposed. The overall\n",
      "procedure is similar to the original k-means method [20], with a key difference that semi-\n",
      "supervised k-means is aware of the labeled data in Dl, and in each step to recompute the\n",
      "cluster assignment, the samples that already have labels will be assigned to the correct cluster\n",
      "regardless of its distance to the nearest cluster centroids.\n",
      "3.2\n",
      "Dataset Partitioning\n",
      "The key challenge in representation learning for ﬁne-grained GCD is that the representa-\n",
      "tion is required to be sensitive to the detailed discriminative traits of different classes. Learn-\n",
      "ing the model by contrasting between examples in the full dataset may not help the model\n",
      "to learn such a discriminative representation. Thus, we take advantage of the self-supervised\n",
      "representations that can roughly cluster the images according to the overall image statistics\n",
      "(e.g. background, object pose, etc.) [1] to perform a preprocess on the full dataset by par-\n",
      "titioning it into k expert sub-datasets. The overall statistics within each sub-dataset will be\n",
      "similar and then the model will naturally learn ﬁne-grained discriminative features to distin-\n",
      "guish between different examples within each sub-dataset. Each of these expert sub-datasets\n",
      "will be expected to reduce different class-irrelevant cues represented by different overall\n",
      "image statistics.\n",
      "Speciﬁcally, We denote the whole training set as D = {(xi,yi)}. The feature vi = f(xi) is\n",
      "extracted from each image xi. The vi extracted by DINO [1] is incapable of distinguishing\n",
      "between the ﬁne-grained classes since there is no supervision during training, but it will\n",
      "provide a rough description of the image so that similar images will be clustered together.\n",
      "Then, the whole D is clustered into K sub-datasets {D1,D2,··· ,DK} using k-means, each\n",
      "containing similar images and will be used for ﬁne-grained category discovery later.\n",
      "3.3\n",
      "Learning discriminative representations\n",
      "Since the images within each of the partitioned sub-dataset only have ﬁne-grained dif-\n",
      "ferences with each other, and each sub-dataset naturally has different global statistics over-\n",
      "all, we use a set of projectors hj(·), j = 1,··· ,K to project features to each corresponding\n",
      "6 Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "sub-spaces in which contrastive learning will be performed. Each projector can be consid-\n",
      "ered an expert projector dedicated to learning ﬁne-grained discriminative features from each\n",
      "sub-dataset. Similar to Vaze et al. [26], we apply both supervised contrastive loss and self-\n",
      "supervised contrastive loss to ﬁne-tune the model. Speciﬁcally, our proposed ﬁne-grained\n",
      "self-supervised contrastive loss is\n",
      "Lu\n",
      "ﬁne = −\n",
      "K\n",
      "∑\n",
      "k=1\n",
      "1\n",
      "|Bk| ∑\n",
      "i∈Bk\n",
      "log\n",
      "exp(hk(vi)·hk(ˆ\n",
      "vi)/τ)\n",
      "∑j 1[ j̸=i] exp(hk(vi)·hk(vj)/τ)\n",
      "(4)\n",
      "where Bk is a minibatch of images sampled from a partitioned dataset Dk, vi and ˆ\n",
      "vi are two\n",
      "views of one same image through data augmentation, and τ is the temperature parameter.\n",
      "The ﬁne-grained supervised contrastive loss is deﬁned similarly\n",
      "Ll\n",
      "ﬁne = −\n",
      "K\n",
      "∑\n",
      "k=1\n",
      "1\n",
      "|Bk| ∑\n",
      "i∈Bk\n",
      "1\n",
      "|N(i)| ∑\n",
      "q∈N(i)\n",
      "log\n",
      "exp(hk(vi)·hk(vq)/τ)\n",
      "∑j 1[ j̸=i] exp(hk(vi)·hk(vj)/τ)\n",
      "(5)\n",
      "where N(i) is the set of indices for images with the same label as the anchor image i.\n",
      "Thus, the overall loss we propose to learn ﬁne-grained features is the combination of two\n",
      "losses deﬁned above\n",
      "Lﬁne = (1−λ)Lu\n",
      "ﬁne +λLl\n",
      "ﬁne\n",
      "(6)\n",
      "Together with the loss from Vaze et al. [26] deﬁned in Eq. (3), which can be viewed as\n",
      "a coarse-grained loss Lcoarse compared to our proposed Lﬁne, our optimization objective is\n",
      "L = Lcoarse +αLﬁne\n",
      "(7)\n",
      "where α is a parameter to balance between our proposed Lﬁne and the original Lcoarse from\n",
      "Vaze et al. [26]. After the representation is learned, we run the semi-supervised k-means\n",
      "algorithm to obtain the cluster assignments of each sample.\n",
      "4\n",
      "Experiments\n",
      "Datasets. We evaluate our method on both generic image classiﬁcation datasets and ﬁne-\n",
      "grained datasets, with a special focus on the performance of the ﬁne-grained image clas-\n",
      "siﬁcation datasets. Following previous works, we choose CIFAR-10/100 [18], ImageNet-\n",
      "100 [5] as the generic image classiﬁcation datasets. For ﬁne-grained datasets we choose\n",
      "CUB-200 [27], Standford Cars [17], FGVC-Aircraft [21], and Oxford-IIIT Pet [22]. These\n",
      "ﬁne-grained datasets contain categories from the same entry level classes, e.g., birds, cars,\n",
      "aircrafts, and pets. These datasets can be more challenging for GCD methods requiring\n",
      "models to learn highly discriminative features [29]. We split the training data into a labeled\n",
      "dataset and an unlabeled dataset by ﬁrst dividing all classes equally into a seen class set and\n",
      "an unseen one then sampling 50% images from the seen classes as unlabeled data so that\n",
      "the unlabeled set Du contains images from both seen classes and unseen classes, while the\n",
      "labeled set only contains seen classes, the splits are presented in Table 1.\n",
      "Evaluation metric. We employ the clustering accuracy (ACC) on the unlabeled set to mea-\n",
      "sure the performance. The evaluation metric is deﬁned as below\n",
      "ACC = max\n",
      "p∈P(yu)\n",
      "1\n",
      "N\n",
      "N\n",
      "∑\n",
      "i=1\n",
      "1{yi = p( ˆ\n",
      "yi)}\n",
      "(8)\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 7\n",
      "Table 1: Our dataset splits in the experiments.\n",
      "Dataset\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "ImageNet-100\n",
      "CUB-200\n",
      "SCars\n",
      "Aircraft\n",
      "Pet\n",
      "Labelled\n",
      "Classes\n",
      "5\n",
      "80\n",
      "50\n",
      "100\n",
      "98\n",
      "50\n",
      "19\n",
      "Images\n",
      "12.5k\n",
      "20k\n",
      "31.9k\n",
      "1498\n",
      "2000\n",
      "1666\n",
      "942\n",
      "Unlabelled\n",
      "Classes\n",
      "10\n",
      "100\n",
      "100\n",
      "200\n",
      "196\n",
      "100\n",
      "37\n",
      "Images\n",
      "37.5k\n",
      "30k\n",
      "95.3k\n",
      "4496\n",
      "6144\n",
      "5001\n",
      "2738\n",
      "where P is the set of all permutations that can match the clustering prediction ˆ\n",
      "yi with the\n",
      "ground-truth label yi, we use the Hungarian algorithm [19] to ﬁnd the best permutation, and\n",
      "N is the number of images in the unlabeled set. Following [26], we use the metric on\n",
      "three different sets, including ‘All’ referring to the entire unlabeled set Du, ‘Old’ referring to\n",
      "instances in Du belonging to classes in Cl and ‘New’ referring to instances in Du belonging\n",
      "to Cu \\Cl.\n",
      "4.1\n",
      "Implementation details\n",
      "We follow the implementation of\n",
      "[26] to use ViT-B-16 [6] as the backbone of our\n",
      "method. We initialize the model with the parameters pretrained by DINO [1] on ImageNet\n",
      "and only ﬁne-tune the ﬁnal transformer block while other blocks are frozen. We implement\n",
      "the projection heads as three layer MLPs following DINO [1], these projection heads will\n",
      "be discarded when testing. The batch size for the entire training dataset is set to 256 and the\n",
      "batch size of all the sub-datasets is set to 32. For the ImageNet dataset, all models are trained\n",
      "for 60 epochs while for other datasets, models are trained for 200 epochs. We set α to be\n",
      "0.1 by default. Similar to [26], we use a base learning rate of 0.1 with a cosine annealing\n",
      "schedule and set λ to 0.35. For a fair comparison with existing methods, we use the same\n",
      "semi-supervised k-means method as [26] to do the evaluation.\n",
      "4.2\n",
      "Comparison with the State-of-the-Art\n",
      "We ﬁrst compare XCon with the state-of-the-art methods on both generic image classiﬁ-\n",
      "cation benchmarks and ﬁne-grained image classiﬁcation benchmarks. The k-means method\n",
      "in the tables refers to running k-means directly on the features extracted from DINO without\n",
      "any further ﬁnetuning. RankStats+ and UNO+ are two methods modiﬁed from two compet-\n",
      "itive baselines for NCD and adopted to the GCD setting, i.e. RankStats [10] and UNO [7].\n",
      "The results on generic image classiﬁcation benchmarks are shown in Table 2. On all\n",
      "the datasets we tested, XCon shows the best performance on ‘All’, showing that our method\n",
      "could improve upon previous works. XCon also achieves comparable results with other\n",
      "methods on the other subsets as ‘Old’ and ‘New’. It should be noticed the best performance\n",
      "on ImageNet-100 ‘New’ subset is achieved by naively running a k-means on DINO features,\n",
      "suggesting that the original features can already represent the unlabeled categories well, and\n",
      "XCon achieves the closest performance compared to this baseline, showing that unlike ex-\n",
      "isting method potentially introducing damage to original feature quality which results in\n",
      "signiﬁcant performance drop, our method can best preserve the high quality of original fea-\n",
      "tures.\n",
      "We present the results on ﬁne-grained image classiﬁcation benchmarks in Table 3. Our\n",
      "method shows the best performance on the ‘All’ and ‘New’ with all four datasets we tested\n",
      "while achieving comparable results on ‘Old’, indicating the effectiveness of our method for\n",
      "ﬁne-grained category discovery.\n",
      "4.3\n",
      "Ablation study\n",
      "We perform the ablation study by adjusting each element of our method to inspect the\n",
      "effectiveness of them. For quicker evaluation, we use two ﬁne-grained datasets, i.e. CUB-\n",
      "8 Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "Table 2: Results on generic datasets.\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "ImageNet-100\n",
      "Method\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "k-means [20]\n",
      "83.6\n",
      "85.7\n",
      "82.5\n",
      "52.0\n",
      "52.2\n",
      "50.8\n",
      "72.7\n",
      "75.5\n",
      "71.3\n",
      "RankStats+\n",
      "46.8\n",
      "19.2\n",
      "60.5\n",
      "58.2\n",
      "77.6\n",
      "19.3\n",
      "37.1\n",
      "61.6\n",
      "24.8\n",
      "UNO+\n",
      "68.6\n",
      "98.3\n",
      "53.8\n",
      "69.5\n",
      "80.6\n",
      "47.2\n",
      "70.3\n",
      "95.0\n",
      "57.9\n",
      "GCD [26]\n",
      "91.5\n",
      "97.9\n",
      "88.2\n",
      "73.0\n",
      "76.2\n",
      "66.5\n",
      "74.1\n",
      "89.8\n",
      "66.3\n",
      "XCon\n",
      "96.0\n",
      "97.3\n",
      "95.4\n",
      "74.2\n",
      "81.2\n",
      "60.3\n",
      "77.6\n",
      "93.5\n",
      "69.7\n",
      "Table 3: Results on ﬁne-grained datasets.\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "FGVC-Aircraft\n",
      "Oxford-Pet\n",
      "Method\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "k-means [20]\n",
      "34.3\n",
      "38.9\n",
      "32.1\n",
      "12.8\n",
      "10.6\n",
      "13.8\n",
      "16.0\n",
      "14.4\n",
      "16.8\n",
      "77.1\n",
      "70.1\n",
      "80.7\n",
      "RankStats+\n",
      "33.3\n",
      "51.6\n",
      "24.2\n",
      "28.3\n",
      "61.8\n",
      "12.1\n",
      "26.9\n",
      "36.4\n",
      "22.2\n",
      "-\n",
      "-\n",
      "-\n",
      "UNO+\n",
      "35.1\n",
      "49.0\n",
      "28.1\n",
      "35.5\n",
      "70.5\n",
      "18.6\n",
      "40.3\n",
      "56.4\n",
      "32.2\n",
      "-\n",
      "-\n",
      "-\n",
      "GCD [26]\n",
      "51.3\n",
      "56.6\n",
      "48.7\n",
      "39.0\n",
      "57.6\n",
      "29.9\n",
      "45.0\n",
      "41.1\n",
      "46.9\n",
      "80.2\n",
      "85.1\n",
      "77.6\n",
      "XCon\n",
      "52.1\n",
      "54.3\n",
      "51.0\n",
      "40.5\n",
      "58.8\n",
      "31.7\n",
      "47.7\n",
      "44.4\n",
      "49.4\n",
      "86.7\n",
      "91.5\n",
      "84.1\n",
      "200 and Standford Cars, and train the model for 100 epochs to ablate the performance.\n",
      "Fine-grained and coarse-grained loss.\n",
      "Table 4 presents the performance of using dif-\n",
      "ferent combinations of loss terms. We observed that with additional supervision from the\n",
      "coarse-grained loss, the ACC is improved by 3.3-4.0% on CUB-200 and 15.4-28.5% on\n",
      "Standford Cars. As combining the ﬁne-grained and coarse-grained losses achieves the best\n",
      "performance, it is proved that our proposed method to learn ﬁne-grained features improves\n",
      "GCD methods’ performance in ﬁne-grained benchmarks.\n",
      "Table 4: Ablation study of ﬁne-grained loss and coarse-grained loss.\n",
      "Lﬁne\n",
      "Lcoarse\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "✓\n",
      "48.0\n",
      "50.5\n",
      "46.8\n",
      "21.3\n",
      "30.6\n",
      "16.8\n",
      "✓\n",
      "49.9\n",
      "53.4\n",
      "48.2\n",
      "37.1\n",
      "57.9\n",
      "27.0\n",
      "✓\n",
      "✓\n",
      "51.8\n",
      "53.8\n",
      "50.8\n",
      "41.0\n",
      "59.1\n",
      "32.2\n",
      "The weight of ﬁne-grained loss.\n",
      "We analyze the choice of the weight α for ﬁne-grained\n",
      "loss in Table 5. We ﬁnd that XCon can consistently outperform the baseline(α = 0) with\n",
      "different α, showing the robust effectiveness of our method. The best result is achieved with\n",
      "α = 0.4 on CUB-200 and with α = 0.2 on Standford Cars.\n",
      "The number of sub-datasets.\n",
      "The effect of the sub-dataset number is illustrated in Ta-\n",
      "ble 6. Although the performance of XCon is consistently better than the baseline’s, it still\n",
      "varies greatly depending on the number of sub-datasets. When K = 2, it can reach the high-\n",
      "est on the ‘Old’ set, but the lowest on the ‘New’ set, that means with two groups, the overall\n",
      "difference between features is not so great inside each group that the model tends to focus\n",
      "more on the existing coarse-grained knowledge learned from the seen classes.\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 9\n",
      "Table 5: Ablation study on the weight α of loss. α = 0 is the baseline(Vaze et al. [26]).\n",
      "α\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "0\n",
      "49.9\n",
      "53.4\n",
      "48.2\n",
      "37.1\n",
      "57.9\n",
      "27.0\n",
      "0.1\n",
      "51.8\n",
      "53.8\n",
      "50.8\n",
      "41.0\n",
      "59.1\n",
      "32.2\n",
      "0.2\n",
      "51.6\n",
      "54.5\n",
      "50.2\n",
      "42.4\n",
      "63.0\n",
      "32.4\n",
      "0.4\n",
      "53.4\n",
      "58.6\n",
      "50.9\n",
      "41.1\n",
      "61.2\n",
      "31.4\n",
      "DINO\n",
      "XCon\n",
      "Figure 3: Feature visualization on CIFAR10 with TSNE.\n",
      "We further visualize the feature spaces with TSNE [25] on CIFAR10 by mapping the\n",
      "features into two dimensions for a more direct qualitative analysis. In Fig. 3, we cluster\n",
      "the unlabeled data and compare results from the initial model(DINO) with ones from our\n",
      "method(XCon). It is clear that the improvement from DINO to our method is signiﬁcant.\n",
      "DINO can cluster the features into 10 groups roughly, but many samples appear in groups\n",
      "that correspond to other classes. In contrast to DINO, with our model, we can see clear\n",
      "boundaries between different groups, and each group is corresponding to one certain cate-\n",
      "gory in CIFAR10.\n",
      "5\n",
      "Conclusion\n",
      "In this paper, we propose XCon to address the problem of generalized category discovery\n",
      "with ﬁne-grained image classiﬁcation benchmarks. XCon ﬁrst partitions the dataset into K\n",
      "sub-dataset using k-means clustering on a self-supervised representation. Each partitioned\n",
      "sub-dataset can be seen as a subset of images that are visually similar and have close coarse-\n",
      "grained representation so that contrastive learning within each of these sub-datasets will\n",
      "force the model to learn ﬁne-grained discriminative features that can help discover ﬁne-\n",
      "grained categories. Experiments on four ﬁne-grained image classiﬁcation benchmarks show\n",
      "clear performance improvements of XCon, validating the effectiveness of our method.\n",
      "Acknowledge\n",
      "The author would like to acknowledge compute support from LunarAI.\n",
      "10Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "Table 6: Ablation study on the number K of split sub-groups.\n",
      "K\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "1\n",
      "49.9\n",
      "53.4\n",
      "48.2\n",
      "37.1\n",
      "57.9\n",
      "27.0\n",
      "2\n",
      "51.4\n",
      "59.3\n",
      "47.4\n",
      "40.9\n",
      "61.0\n",
      "31.1\n",
      "4\n",
      "51.7\n",
      "54.6\n",
      "50.2\n",
      "39.8\n",
      "55.3\n",
      "32.3\n",
      "6\n",
      "50.3\n",
      "51.9\n",
      "49.5\n",
      "42.1\n",
      "60.7\n",
      "33.1\n",
      "8\n",
      "51.8\n",
      "53.8\n",
      "50.8\n",
      "41.0\n",
      "59.1\n",
      "32.2\n",
      "References\n",
      "[1] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bo-\n",
      "janowski, and Armand Joulin. Emerging properties in self-supervised vision transform-\n",
      "ers. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n",
      "pages 9650–9660, 2021.\n",
      "[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple\n",
      "framework for contrastive learning of visual representations. In International confer-\n",
      "ence on machine learning, pages 1597–1607. PMLR, 2020.\n",
      "[3] Xinlei Chen, Saining Xie, and Kaiming He.\n",
      "An empirical study of training self-\n",
      "supervised vision transformers. In Proceedings of the IEEE/CVF International Con-\n",
      "ference on Computer Vision, pages 9640–9649, 2021.\n",
      "[4] Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, Gang Niu,\n",
      "Mingyuan Zhou, and Masashi Sugiyama. Meta discovery: Learning to discover novel\n",
      "classes given very limited data. In International Conference on Learning Representa-\n",
      "tions, 2021.\n",
      "[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\n",
      "large-scale hierarchical image database. In 2009 IEEE conference on computer vision\n",
      "and pattern recognition, pages 248–255. Ieee, 2009.\n",
      "[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\n",
      "Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\n",
      "vain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\n",
      "at scale. arXiv preprint arXiv:2010.11929, 2020.\n",
      "[7] Enrico Fini, Enver Sangineto, Stéphane Lathuilière, Zhun Zhong, Moin Nabi, and Elisa\n",
      "Ricci. A uniﬁed objective for novel class discovery. In Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision, pages 9284–9292, 2021.\n",
      "[8] Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to discover novel visual\n",
      "categories via deep transfer clustering. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 8401–8409, 2019.\n",
      "[9] Kai Han, Sylvestre-Alvise Rebufﬁ, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew\n",
      "Zisserman. Automatically discovering and learning new visual categories with rank-\n",
      "ing statistics. In International Conference on Learning Representations, 2020. URL\n",
      "https://openreview.net/forum?id=BJl2_nVFPB.\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY11\n",
      "[10] Kai Han, Sylvestre-Alvise Rebufﬁ, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew\n",
      "Zisserman. Autonovel: Automatically discovering and learning novel visual categories.\n",
      "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n",
      "[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum con-\n",
      "trast for 393 unsupervised visual representation learning. In Conference on Computer\n",
      "Vision and Pattern, volume 394, 2019.\n",
      "[12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.\n",
      "Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pages 16000–16009, 2022.\n",
      "[13] Yen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer\n",
      "across domains and tasks. arXiv preprint arXiv:1711.10125, 2017.\n",
      "[14] Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, and Zsolt Kira. Multi-\n",
      "class classiﬁcation without multi-class labels. arXiv preprint arXiv:1901.00544, 2019.\n",
      "[15] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane\n",
      "Larlus. Hard negative mixing for contrastive learning. Advances in Neural Information\n",
      "Processing Systems, 33:21798–21809, 2020.\n",
      "[16] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip\n",
      "Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning.\n",
      "Advances in Neural Information Processing Systems, 33:18661–18673, 2020.\n",
      "[17] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for\n",
      "ﬁne-grained categorization. In 4th International IEEE Workshop on 3D Representation\n",
      "and Recognition (3dRR-13), Sydney, Australia, 2013.\n",
      "[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny\n",
      "images. Technical report, 2009.\n",
      "[19] Harold W Kuhn. The hungarian method for the assignment problem. Naval research\n",
      "logistics quarterly, 2(1-2):83–97, 1955.\n",
      "[20] James MacQueen. Some methods for classiﬁcation and analysis of multivariate obser-\n",
      "vations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics\n",
      "and Probability, 1967.\n",
      "[21] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.\n",
      "Fine-grained visual classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n",
      "[22] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and\n",
      "dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages\n",
      "3498–3505. IEEE, 2012.\n",
      "[23] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-\n",
      "time object detection with region proposal networks. Advances in neural information\n",
      "processing systems, 28, 2015.\n",
      "12Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "[24] Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric\n",
      "Xing.\n",
      "Un-mix: Rethinking image mixtures for unsupervised visual representation\n",
      "learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),\n",
      "2022.\n",
      "[25] Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. The Journal\n",
      "of Machine Learning Research, 15(1):3221–3245, 2014.\n",
      "[26] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category\n",
      "discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-\n",
      "tern Recognition, pages 7492–7501, 2022.\n",
      "[27] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The\n",
      "caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California\n",
      "Institute of Technology, 2011.\n",
      "[28] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learn-\n",
      "ing via non-parametric instance discrimination. In Proceedings of the IEEE conference\n",
      "on computer vision and pattern recognition, pages 3733–3742, 2018.\n",
      "[29] Bingchen Zhao and Kai Han. Novel visual category discovery with dual ranking statis-\n",
      "tics and mutual knowledge distillation. Advances in Neural Information Processing\n",
      "Systems, 34:22982–22994, 2021.\n",
      "[30] Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, and Nicu Sebe.\n",
      "Neighborhood contrastive learning for novel class discovery. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10867–\n",
      "10875, 2021.\n",
      "[31] Zhun Zhong, Linchao Zhu, Zhiming Luo, Shaozi Li, Yi Yang, and Nicu Sebe. Open-\n",
      "mix: Reviving known knowledge for discovering novel visual categories in an open\n",
      "world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 9462–9470, 2021.\n",
      "[32] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improv-\n",
      "ing contrastive learning by visualizing feature transformation. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision, pages 10306–10315, 2021.\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY13\n",
      "A\n",
      "Different self-supervised representations\n",
      "The self-supervised representation provides us the information to partition the training\n",
      "data into k expert sub-datasets, so we analyze the performance of our method by ﬁne-tuning\n",
      "different pretrained representations of other self-supervised ViT models, i.e. MoCo v3 [3]\n",
      "and MAE [12]. We initialize the ViT-B-16 model [6] with the parameters pretrained on\n",
      "ImageNet-1k by MoCo v3 for 300 epochs and by MAE for 800 epochs respectively. The\n",
      "result in Table 7 shows that with the self-supervised representation of DINO [1], our method\n",
      "performs 7.3 −20.7% better than the other two on CUB-200 and 1.8 −22.4% better on\n",
      "Standford-Cars. We observe that DINO still shows the best performance on clustering the\n",
      "data based on class-irrelevant informations.\n",
      "Table 7: Results with different self-supervised representations.\n",
      "self-supervised\n",
      "ViT model\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "DINO\n",
      "51.8\n",
      "53.8\n",
      "50.8\n",
      "41.0\n",
      "59.1\n",
      "32.2\n",
      "MoCo v3\n",
      "37.6\n",
      "42.8\n",
      "35.1\n",
      "24.7\n",
      "36.7\n",
      "18.9\n",
      "MAE\n",
      "35.5\n",
      "46.5\n",
      "30.1\n",
      "38.7\n",
      "56.0\n",
      "30.4\n",
      "B\n",
      "Estimating the number of classes\n",
      "As a more realistic scenario, the prior knowledge of the number of classes is unknown in\n",
      "the GCD. We follow the method in [26] to estimate the number of classes in the unlabeled\n",
      "dataset by leveraging the information of the labeled dataset. We compare our estimated\n",
      "number of classes in unlabeled data\n",
      "\f",
      "\n",
      "\f",
      " ˆ\n",
      "Cu\f",
      "\n",
      "\f",
      " with the ground truth number of classes in unlabeled\n",
      "data |Cu| in Table 8. We ﬁnd that on Standfor-Cars and FGVC-Aircraft, the number of classes\n",
      "estimated by our method is signiﬁcantly closer to the ground truth compared with GCD [26].\n",
      "Our method tends to show better performance on ﬁne-grained datasets, given that the dataset\n",
      "partitioning can help the model learn more discriminative features when facing the more\n",
      "challenging datasets that have little obvious difference.\n",
      "Table 8: Estimation of the number of classes in unlabeled data.\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "ImageNet-100\n",
      "CUB-200\n",
      "Standford-Cars\n",
      "FGVC-Aircraft\n",
      "Oxford-Pet\n",
      "Ground truth\n",
      "10\n",
      "100\n",
      "100\n",
      "200\n",
      "196\n",
      "100\n",
      "37\n",
      "GCD [26]\n",
      "9\n",
      "100\n",
      "109\n",
      "231\n",
      "230\n",
      "80\n",
      "34\n",
      "XCon\n",
      "8\n",
      "97\n",
      "109\n",
      "236\n",
      "206\n",
      "101\n",
      "34\n",
      "C\n",
      "Performance with estimated class number\n",
      "We use the class number estimated in Table 8 to evaluate our method, displaying the\n",
      "performance of our method when the unlabeled class number is unavailable. We report the\n",
      "results on generic image classiﬁcation benchmarks in Table 9 and the results on ﬁne-grained\n",
      "image classiﬁcation benchmarks in Table 10. With our estimated class number\n",
      "\f",
      "\n",
      "\f",
      " ˆ\n",
      "Cu\f",
      "\n",
      "\f",
      ", our\n",
      "method performs better on Standford-Cars and also reaches comparable results on the other\n",
      "ﬁve datasets except CIFAR10, which shows that our method is also promising under the\n",
      "more realistic condition.\n",
      "D\n",
      "Ablation on contrastive ﬁne-tuning\n",
      "We further ablate the components of contrastive loss in Table 11. We ﬁnd that only with\n",
      "unsupervised contrastive loss, i.e. λ = 0, the ACC drops 21.5 −23.6% on CUB-200 and\n",
      "22.2 −46.6% on Standford-Cars, which means the combination of supervised contrastive\n",
      "14Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY\n",
      "Table 9: Results on generic datasets with our estimated class number.\n",
      "known Cu\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "ImageNet-100\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "\u0013\n",
      "96.0\n",
      "97.3\n",
      "95.4\n",
      "74.2\n",
      "81.2\n",
      "60.3\n",
      "77.6\n",
      "93.5\n",
      "69.7\n",
      "\u0017\n",
      "70.1\n",
      "97.4\n",
      "56.5\n",
      "72.5\n",
      "80.3\n",
      "56.8\n",
      "75.6\n",
      "91.5\n",
      "67.6\n",
      "Table 10: Results on ﬁne-grained datasets with our estimated class number.\n",
      "known Cu\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "FGVC-Aircraft\n",
      "Oxford-Pet\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "\u0013\n",
      "52.1\n",
      "54.3\n",
      "51.0\n",
      "40.5\n",
      "58.8\n",
      "31.7\n",
      "47.7\n",
      "44.4\n",
      "49.4\n",
      "86.7\n",
      "91.5\n",
      "84.1\n",
      "\u0017\n",
      "51.0\n",
      "57.8\n",
      "47.6\n",
      "41.3\n",
      "58.8\n",
      "32.8\n",
      "46.1\n",
      "47.6\n",
      "45.3\n",
      "82.1\n",
      "81.7\n",
      "82.4\n",
      "loss and unsupervised contrastive loss with the balanced parmeter λ = 0.35 is necessary and\n",
      "can reach the best performance.\n",
      "Table 11: Ablation study of contrastive loss.\n",
      "λ\n",
      "CUB-200\n",
      "Stanford-Cars\n",
      "All\n",
      "Old\n",
      "New\n",
      "All\n",
      "Old\n",
      "New\n",
      "0\n",
      "29.6\n",
      "30.2\n",
      "29.3\n",
      "10.8\n",
      "12.5\n",
      "10.0\n",
      "0.35\n",
      "51.8\n",
      "53.8\n",
      "50.8\n",
      "41.0\n",
      "59.1\n",
      "32.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import fitz  \n",
    "\n",
    "pdf_path = \"XCon Learning with Experts for Fine-grained Category Discovery.pdf\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  \n",
    "    all_text = \"\"  \n",
    "\n",
    "    for page in doc:\n",
    "        all_text += page.get_text() \n",
    "\n",
    "    doc.close() \n",
    "    return all_text\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_tokens:  2636\n",
      "Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 1                XCon: Learning with Experts forFine-grained Category Discovery. We address the problem of generalized category discovery (GCD) in this paper. Expert-Contrastive Learning (XCon) is a novel method to mine useful information from the images. It uses k-means clustering and then performing contrastive learning on each sub-dataset to learn discriminative features. Experiments show a clear improved performance over the previous best methods, demonstrating the effectiveness of our method. ations are available, such as image recogni-tion [5] and object detection. However, collecting a dataset at scales like ImageNet or COCO is not always possible. The problem of generalized category discovery was recently formalized in [26] The aim is to discover categories within the unlabeled data by leveraging the information. Clusters formed by DINO features are mainly based on the class irrelevant cues, e.g., background and objectpose. The features learned by our method could cluster the images based on. the correct cues, such as object classes. The images in each row represent a cluster in k-means. n generic category discovery since expertsare interested in the ﬁne-grained concepts in real applications. We have ob-served that an unsupervised representation (e.g. DINO) could cluster the data based on classirrelevant cues such as the object pose or the background, see the left part of Fig. 1. amed Expert Contrastive Learning(XCon) method. In our proposed XCon method, we partition the data into k expert sub-datasets by directly performing k-means clustering on self-supervised representations. Each of these sub- datasets can be viewed as an expert dataset used to eliminate the negative inﬂuence introduced by certain kinds of class-irrelevant cues. Novel Category Discovery aims to discover new object categories by transferring knowledge learned from a set of relevant but different seen classes. We proposed a method that can learn discriminative features for ﬁne-grained category discovery by partitioning the data into k sub-datasets. We validated the effectiveness of our proposed method by setting a new state-of-the-artperformance on seven tested category discovery benchmarks.  Contrastive learning has beenexplored under this NCD problem by NCL [30], showing strong performance. We use k-means grouping on a self-supervised feature to provide infor-                mative pairs for contrastive learning. Our work also builds on a newly proposed setting named Generalized Category Discov-                ery. Contrastive learning has been showing to be effective for learning representations. Our focus is to learn representations that can be used to discover novel ﬁne-grained categories. By partitioning the dataset into k sub-datasets using MixUp, we can create informative contrastive pairs. -means, examples within each sub-dataset will be similar so that the model will be forced to learn more discriminative features. Compared to previous GCD methods with contrastivelearning [26], our method shows clear performance improvements. We partition the dataset into K sub- datasets using k-Means clustering. We perform joint contrastive representation learning on each of the partitioned sub- datasets. In GCD, the training dataset contains two parts, a labeled dataset Dl and an unlabeled dataset Du. The goal of GCD is to learn a model to categorize the instances in Du by leveraging the information from Dl. It has been shown that self-supervised ViT features could be a good initialization for representation learning in GCD. In this section, we review the method proposed in Vaze et al. [26] for GCD, which consists of two parts, representation learning and class assignment. We propose a simple method that partitions the dataset into k sub-datasets by using k-means on self-supervised features. The supervised contrastive loss is deﬁned as de）1, where N(i) is the set of indices of images in the minibatch that have the same label yi with the anchor image i. The overall procedure is similar to the original k-means method [20] for class assignments. The key challenge in representation learning for ﬁne-grained GCD is that the representa-                tion is required to be sensitive to the detailed discriminative traits of different classes. We take advantage of the self-supervisedrepresentations that can roughly cluster the images according to the overall image statistics. Ture vi = f(xi) is extracted from each image xi. The vi extracted by DINO [1] is incapable of distinguishing between the ﬁne-grained classes since there is no supervision during training. It will provide a rough description of the image so that similar images will be clustered together. The whole D is clustered into K sub-datasets {D1,D2,···,DK} using k-means. We apply both supervised contrastive loss and self-supervised contrastive lost to ﬁne-tune the model. The overall loss we propose to learn features is the combination of twolosses deﬁned above. The loss from Vaze et al. [26] can be viewed as a coarse-grained version of the loss from Eq. (3) loss Lcoarse compared to our proposed Lﬁne. After the representation is learned, we run the semi-supervised k-means                algorithm to obtain the cluster assignments of each sample. We evaluate our method on both generic image classi ﬁcation datasets and ﬃne-grained datasets. plit the training data into a labeled and an unlabeled dataset. We employ the clustering accuracy (ACC) on the unlabelled set to mea-sure the performance. The evaluation metric is deﬁned as below:ACC = maxp(yu) P is the set of all permutations that can match the predicts. We use ViT-B-16 [6] as the backbone of our method. The batch size for the entire training dataset is set to 256. For the ImageNet dataset, all models are trained for 60 epochs. We implement the projection heads as three layer MLPs. We compare XCon with the state-of-the-art methods on both generic image classiﬁ-cation benchmarks and ﬁne-grained images. For a fair comparison with existing methods, we use the same.semi-supervised k-means method as [26] to do the evaluation. The results are shown in Table 2. We present the results on ﬁne-grained image classiﬁcation benchmarks in Table 3. The best performance on ImageNet-100 ‘New’ subset is achieved by naively running a k-means on DINO features, suggesting that the original features can already represent the unlabeled categories well. Table 2: Results on generic datasets. CIFAR10, 100, 200 and Standford Cars, and train the model for 100 epochs to ablate the performance. Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY. The ACC is improved by 3.3-4.0% on CUB-200 and 15.4-28.5% on Standford Cars. XCon can consistently outperform the baseline(α = 0) with                different α, showing the robust effectiveness of our method. We analyze the choice of the weight α for ﬁne-grained loss in Table 5. XCon is consistently better than the baseline’s, but it still varies greatly depending on the number of sub-datasets. When K = 2, it can reach the high-                est on the ‘Old’ set, but the lowest on the New. The overall difference between features is not so great inside each group. features into two dimensions for a more direct qualitative analysis. In contrast to DINO, with our model, we can see clearboundaries between different groups, and each group is corresponding to one certain cate-                gory in CIFAR10.5. We propose XCon to address the problem of generalized category discovery with a self-supervised representation. model to learn ﬁne-grained discriminative features that can help discover categories. Experiments on four image classiﬁcation benchmarks show clear performance improvements of XCon, validating the effectiveness of our method. The author would like to acknowledge compute support from LunarAI. An empirical study of training self-supervised vision transformers. A simpleframework for contrastive learning of visual representations. A large-scale hierarchical image database. A new way to learn to discover novel classes given very limited data. A better way to teach people to use computer vision. -vain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXIV:2010.11929, 2020. The IEEE/CVF International.Conference on Computer Vision will be held in 2021. Masked autoencoders are scalable vision learners. Multi-class classiﬁcation without multi-class labels. Learning to cluster in order to transfer seamlessly across domains and tasks. Autonovel: Automatically discovering and learning novel visual categories. Masked Autoencoder: Scalable vision learners are scalable. Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip                Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Advances in Neural InformationProcessing Systems, 33:21798–21809, 2020. The Journa.sium on Mathematical Statistics and Probability, 1967. The. Journo’s. preprint arXiv:1306.5151, 2013. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2022. The California Institute of Technology created the birds-200-2011 dataset. The dataset was used to train a new type of machine-learning system. The results of the study were published in the journal of Machine Learning Research, 15(1):3221–3245, 2014. ning for novel class discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10867–10875, 2021. The self-supervised representation provides us the information to partition the training data into k expert sub-datasets, so we analyze the performance of our method by ﬁne-tuning                different pretrained representations. Supervised ViT models, i.e. MoCo v3 [3] and MAE [12], were used. We observe that DINO still shows the best performance on clustering the data based on class-irrelevant informations. The self-supervised representation of DINO [1] performs 7.3 −20.7% better than the other two. Our method tends to show better performance on ﬁne-grained datasets. We report the results on the Standfor-Cars and FGVC-Aircraft. dataset. We compare our estimated.number of classes in unlabeled data with the ground truth number of classes. in unlabelled data. With our estimated class number, our method performs better on Standford-Cars and also reaches comparable results on the other datasets except CIFAR10. We further ablate the components of contrastive loss in Table 11. We find that only with the combination of supervised contrastive. and unsupervised contrastive, i.e. λ = 0, the ACC drops 21.5 −23.6% on CUB-200 and 22.2 −46.6%. The method is also promising under the more realistic condition. s with our estimated class number. Unsupervised contrastive loss with the balanced parmeter. λ = 0.35 is necessary and can reach the best performance. CuCUB-200                Stanford-Cars                FGVC-Aircraft                Oxford-Pet.\n",
      "CPU times: user 20min 53s, sys: 5.8 s, total: 20min 59s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "def chunk_text(text, max_tokens=1024):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), max_tokens):\n",
    "        chunks.append(text[i:i+max_tokens])\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    summaries = []\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(inputs.input_ids, max_length=150, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "def concatenate_summaries(summaries):\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "# Your long text\n",
    "long_text = pdf_text\n",
    "\n",
    "# Step 1: Chunk the text\n",
    "text_chunks = chunk_text(long_text)\n",
    "\n",
    "# Step 2: Summarize each chunk\n",
    "chunk_summaries = summarize_chunks(text_chunks)\n",
    "\n",
    "# Step 3: Concatenate the summaries\n",
    "final_summary = concatenate_summaries(chunk_summaries)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokens = tokenizer.tokenize(final_summary)\n",
    "number_of_tokens = len(tokens)\n",
    "print(\"number_of_tokens: \", number_of_tokens)\n",
    "\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_tokens:  2258\n",
      "\n",
      "We address the problem of generalized category discovery (GCD) in this paper, i.e. leveraging the information from a set of seen classes. The seen classes can be seen as an implicit criterion of classes, which makes this setting different from unsupervised clustering where the cluster criteria may be ambiguous. We mainly Concern the problem. of discovering categories within a ﬁne-grained dataset.\n",
      "Expert-Contrastive Learning (XCon) is a novel method to mine useful information from images. It uses k-means clustering and contrastive learning on each sub-datasets to learn discriminative features. Experiments on ﬁne-grained datasets show a clear improved performance over the previous best methods.\n",
      " collecting a dataset at scales like ImageNet or COCO is not always possible. The problem of generalized category discovery was recently formalized in [26] The aim is to discover categories within the unlabeled data by leveraging the information.\n",
      "Clusters formed by DINO features are mainly based on the class irrelevant cues, e.g., background and objectpose. The features learned by our method could cluster the images. after ﬁne-tuning with our method. The images in each row represent a cluster in k-means.\n",
      "We propose a simple yet effective method to boost the performance of generalized category discovery on ﬁne-grained data n. The main challenge is the large inter-class similarity and the intra-class                variance. Different classes may require the model to learn more discriminative features to distinguish.\n",
      "In our proposed XCon method, we partition the data into k expert sub-datasets by directly performing k-means clustering on self-supervised representations. Each of these sub- datasets can be viewed as an expert dataset used to eliminate the negative inﬂuence introduced by certain kinds of class-irrelevant cues.\n",
      "Novel Category Discovery (NCD) aims to discover new object categories by transferring knowledge learned from a set of relevant but different seen classes. We proposed a method that can learn discriminative features for ﬁne-grained category discovery by partitioning the data into k sub-datasets.\n",
      " Contrastive learning has beenexplored under this NCD problem by NCL [30], showing strong performance. The key difference with previous works is that we use k-means grouping on a self-supervised feature. We also build on a newly proposed setting named Generalized Category Discov-                ery.\n",
      "Contrastive learning has been showing to be effective for learning representations. The focus is to learn representations that can be used to discover novel ﬁne-grained categories. By partitioning the dataset into k sub-datasets using the instance discrimination pretext, we can create informative contrastive pairs.\n",
      "We partition the dataset into K sub-datasets using k-means clustering the DINO [1] pretrained representations. We perform joint contrastive representation learning on each of the partitioned sub- datasets D1 ...DK as well as on the full dataset D0.\n",
      "In GCD, the training dataset contains two parts, a labeled dataset Dl and an unlabeled dataset Du. Cl and Cu are composed of both seen and unseen classes, thus Cl ⊆Cu. The goal of GCD is to learn a model to categorize the instances in Du by leveraging the information from Dl.\n",
      "In this section, we review the method proposed in Vaze et al. [26] for GCD, which consists of two parts, representation learning and class assignment. We propose a simple method that partitions the dataset into k sub-datasets by using k-means on self-supervised features.\n",
      "For class assignments, the semi-supervised k-means method is proposed. The overall procedure is similar to the original k-Means method [20] The method is based on the Dl-Dl cluster partitioning model.\n",
      "The key challenge in representation learning for ﬁne-grained GCD is that the representa-                tion is required to be sensitive to the detailed discriminative traits of different classes. We take advantage of the self-supervisedrepresentations that can roughly cluster the images according to the overall image statistics.\n",
      "The whole D is clustered into K sub-datasets using k-means. The vi extracted by DINO [1] is incapable of distinguishing between the ﬁne-grained classes since there is no supervision during training, but it will provide a rough description of the image.\n",
      "We apply both supervised contrastive loss and self-supervised contrastive lost to ﬁne-tune the model. The overall loss we propose to learn  features is the combination of twolosses deﬁned above.\n",
      "We evaluate our method on both generic image classiﬁcation datasets and ﬁne-grained datasets. We choose CIFAR-10/100, ImageNet-100, Standford Cars, FGVC-Aircraft, and Oxford-IIIT Pet. These datasets contain categories from the same entry level classes, e.g., birds, cars, aircrafts, and pets.\n",
      "The dataset splits are presented in Table 1. We employ the clustering accuracy (ACC) on the unlabeled set to mea-sure the performance. The evaluation metric is deﬁned as below:ACC = maxp(yu) P is the set of all permutations that can match the predictio.\n",
      "We use ViT-B-16 [6] as the backbone of our method. The batch size for the entire training dataset is set to 256. For the ImageNet dataset, all models are trained for 60 epochs. We implement the projection heads as three layer MLPs.\n",
      "We compare XCon with the state-of-the-art methods on both generic image classiﬁ-cation benchmarks and ﬁne-grained images. For a fair comparison with existing methods, we use the same.semi-supervised k-means method as [26] to do the evaluation. The results are shown in Table 2.\n",
      "We present the results on ﬁne-grained image classiﬁcation benchmarks in Table 3. Our method shows the best performance on the ‘All’ and ‘New’ with all four datasets we tested. The best performance is achieved by naively running a k-means on DINO features.\n",
      "CIFAR10, CIFAR100, and CIFar100.ImageNet-100 have been used to train the model for 100 epochs to ablate the performance. The results are shown in Table 2 and 3 of the report.\n",
      "The ACC is improved by 3.3-4.0% on CUB-200 and 15.4-28.5% on Standford Cars. XCon can consistently outperform the baseline(α = 0) with different α, showing the robust effectiveness of our method.\n",
      "XCon is consistently better than the baseline’s, but it still varies greatly depending on the number of sub-datasets. When K = 2, XCon can reach the high-                est on the ‘Old’ set, but the lowest on the New. With two groups, the overall.difference between features is not so great inside each group.\n",
      "In Fig. 3, we cluster the unlabeled data and compare results from the initial model(DINO) with ones from our method(XCon) It is clear that the improvement from DINO to our method is signiﬁcant. DINO can cluster the features into 10 groups roughly, but many samples appear in groups that correspond to other classes.\n",
      "XCon can learn ﬁne-grained discriminative features to help discover categories. Experiments on four image classiﬁcation benchmarks show clear performance improvements of XCon. The author would like to acknowledge compute support from LunarAI.\n",
      "An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 9640–9649, 2021. A simpleframework for contrastive learning of visual representations. A large-scale hierarchical image database.\n",
      "An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv preprint arXiv:2010.11929, 2020. -vain Gelly, et al. A uniﬁed objective for novel class discovery. In Proceedings of the IEEE/CVF International. Conference on Computer Vision, 2021.\n",
      "Automatically discovering and learning novel visual categories. Masked autoencoders are scalable vision learners. Multi-class classiﬁcation without multi-class labels. Learning to cluster in order to transfer knowledge across domains and tasks.\n",
      "Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip                Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Hard negative mixing for contrastive learning. Advances in Neural Information Processing Systems, 33:21798–21809, 2020.\n",
      "The Journa. sium on Mathematical Statistics and Probability, 1967. ArXiv preprint arXiv:1306.5151, 2013. Advances in neural information processing systems, 28, 2015.\n",
      "The California Institute of Technology created the birds-200-2011 dataset. The dataset was used to train a new type of machine-learning system. The results of the study were published in the journal of Machine Learning Research, 15(1):3221–3245, 2014.\n",
      "The self-supervised representation provides us the information to partition the training data into k expert sub-datasets. We analyze the performance of our method by ﬁne-tuning                different pretrained representations of other self-Supervised representations.\n",
      "DINO still shows the best performance on clustering the data based on class-irrelevant informations. We follow the method in [26] to estimate the number of classes in the unlabeleddataset by leveraging the information of the labeled data.\n",
      "Our method tends to show better performance on ﬁne-grained datasets. We report the results on the Standfor-Cars and FGVC-Aircraft dataset. The number of classes estimated by our method is signiﬁcantly closer to the ground truth compared with GCD.\n",
      "With our estimated class number, our method performs better on Standford-Cars and also reaches comparable results on the other datasets except CIFAR10. We further ablate the components of contrastive loss in Table 11. We find that only with the combination of supervised contrastive and unsupervised loss, i.e. λ = 0, the ACC drops 21.5 −23.6% on CUB-200 and 22.2 −46.6%.\n",
      "Table 11: Ablation study of contrastive loss. s with our estimated class number. The balanced parmeter λ = 0.35 is necessary and can reach the best performance. CuCUB-200                Stanford-Cars                FGVC-Aircraft                Oxford-Pet. s.\n",
      "CPU times: user 18min 3s, sys: 4.42 s, total: 18min 7s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import pipeline, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_text(text, max_chunk=1024):\n",
    "    chunks = [text[i:i+max_chunk] for i in range(0, len(text), max_chunk)]\n",
    "    summarized_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=200, min_length=50, do_sample=False)\n",
    "        summarized_text +=  \"\\n\"\n",
    "        summarized_text += summary[0]['summary_text']\n",
    "    return summarized_text\n",
    "\n",
    "\n",
    "summary = \"\"\n",
    "summary = summarize_text(pdf_text)\n",
    "tokens = tokenizer.tokenize(summary)\n",
    "number_of_tokens = len(tokens)\n",
    "print(\"number_of_tokens: \", number_of_tokens)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10001f316be94925a9830662e311d2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navdev2/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385e8744aaa342f988f79a573a14e769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/161 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff464458a3240bbbf431e91d46baf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fab8fc32394025abc8a262886a497f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c92e02071bb4b0a9cece5eb0134a183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e5471b655843f88c9edf54f3a4ca7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease convert the following text into a presentation. Give title and content for each slide. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39msummary},\n\u001b[1;32m     15\u001b[0m ]\n\u001b[1;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     17\u001b[0m     messages,\n\u001b[1;32m     18\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:257\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    255\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 257\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    259\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"h2oai/h2o-danube-1.8b-chat\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# We use the HF Tokenizer chat template to format each message\n",
    "# https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Please convert the following text into a presentation. Give title and content for each slide. \" +summary},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "res = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.7 ms, sys: 7.98 ms, total: 79.6 ms\n",
      "Wall time: 99.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pptx import Presentation\n",
    "\n",
    "def add_slide(prs, title, content):\n",
    "    slide_layout = prs.slide_layouts[1]  # Use slide layout index 1 for title slide\n",
    "    slide = prs.slides.add_slide(slide_layout)\n",
    "    title_placeholder = slide.shapes.title\n",
    "    content_placeholder = slide.placeholders[1]\n",
    "\n",
    "    title_placeholder.text = title\n",
    "    content_placeholder.text = content\n",
    "\n",
    "# Create a PowerPoint presentation object\n",
    "prs = Presentation()\n",
    "\n",
    "# Add slides with titles and content\n",
    "slides = [\n",
    "    (\"Introduction\", \"Paper Title: XCon: Learning with Experts for Fine-grained Category Discovery\\n\\nProblem Addressed: Generalized Category Discovery (GCD)\\n\\nMethodology: Expert-Contrastive Learning (XCon)\\n\\nKey Results: Improved performance over previous methods\"),\n",
    "    (\"Background and Motivation\", \"Challenge: Generic category discovery requires large datasets like ImageNet or COCO, which may not always be feasible.\\n\\nFormalization of GCD: Leveraging unlabeled data to discover categories, focusing on fine-grained concepts.\\n\\nLimitation of Existing Approaches: Unsupervised representations may cluster data based on irrelevant cues.\\n\\nProposed Solution: Expert Contrastive Learning (XCon) to eliminate negative influences and discover fine-grained categories effectively.\"),\n",
    "    (\"Methodology Overview\", \"XCon Method: Partition data into k expert sub-datasets using k-means clustering.\\n\\nEach sub-dataset treated as an expert dataset to eliminate negative influences.\\n\\nObjective: Learn discriminative features for fine-grained category discovery.\"),\n",
    "    (\"Contrastive Learning in XCon\", \"Utilizing k-means grouping on self-supervised features for informative contrastive pairs.\\n\\nJoint contrastive representation learning on partitioned sub-datasets.\\n\\nClear performance improvements over previous GCD methods with contrastive learning.\"),\n",
    "    (\"Representation Learning Challenges\", \"Challenge: Representations need to be sensitive to detailed discriminative traits.\\n\\nLeveraging self-supervised representations for rough clustering based on overall image statistics.\\n\\nProposed approach: Supervised and self-supervised contrastive loss to fine-tune the model.\"),\n",
    "    (\"Evaluation Metrics\", \"Splitting training data into labeled (Dl) and unlabeled (Du) datasets.\\n\\nMeasuring performance using clustering accuracy (ACC) on the unlabeled set.\"),\n",
    "    (\"Experimental Setup\", \"Backbone: ViT-B-16\\n\\nBatch size: 256\\n\\nTraining epochs: 60 for ImageNet dataset\\n\\nImplementation: Projection heads as three-layer MLPs\"),\n",
    "    (\"Results on Generic Datasets\", \"Comparison with state-of-the-art methods on CIFAR10, 100, 200, and Stanford Cars.\\n\\nXCon consistently outperforms baseline methods, demonstrating robust effectiveness.\"),\n",
    "    (\"Results on Fine-grained Datasets\", \"Performance improvements on CUB-200 and Stanford Cars benchmarks.\\n\\nXCon's effectiveness across different α values analyzed.\"),\n",
    "    (\"Qualitative Analysis\", \"Visualization of features using t-SNE for qualitative comparison.\\n\\nClear boundaries between different groups with XCon, corresponding to specific categories.\"),\n",
    "    (\"Conclusion\", \"Proposal of XCon for generalized category discovery with self-supervised representation.\\n\\nImproved performance on image classification benchmarks, validating the method's effectiveness.\"),\n",
    "    (\"Acknowledgments\", \"Acknowledgment of compute support from LunarAI.\"),\n",
    "    (\"References\", \"Relevant papers and resources cited in the presentation for further reading.\")\n",
    "]\n",
    "\n",
    "for slide_title, slide_content in slides:\n",
    "    add_slide(prs, slide_title, slide_content)\n",
    "\n",
    "# Save the presentation\n",
    "prs.save(\"presentation.pptx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
