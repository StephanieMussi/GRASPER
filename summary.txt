Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY 1                XCon: Learning with Experts forFine-grained Category Discovery. We address the problem of generalized category discovery (GCD) in this paper. Expert-Contrastive Learning (XCon) is a novel method to mine useful information from the images. It uses k-means clustering and then performing contrastive learning on each sub-dataset to learn discriminative features. Experiments show a clear improved performance over the previous best methods, demonstrating the effectiveness of our method. ations are available, such as image recogni-tion [5] and object detection. However, collecting a dataset at scales like ImageNet or COCO is not always possible. The problem of generalized category discovery was recently formalized in [26] The aim is to discover categories within the unlabeled data by leveraging the information. Clusters formed by DINO features are mainly based on the class irrelevant cues, e.g., background and objectpose. The features learned by our method could cluster the images based on. the correct cues, such as object classes. The images in each row represent a cluster in k-means. n generic category discovery since expertsare interested in the ﬁne-grained concepts in real applications. We have ob-served that an unsupervised representation (e.g. DINO) could cluster the data based on classirrelevant cues such as the object pose or the background, see the left part of Fig. 1. amed Expert Contrastive Learning(XCon) method. In our proposed XCon method, we partition the data into k expert sub-datasets by directly performing k-means clustering on self-supervised representations. Each of these sub- datasets can be viewed as an expert dataset used to eliminate the negative inﬂuence introduced by certain kinds of class-irrelevant cues. Novel Category Discovery aims to discover new object categories by transferring knowledge learned from a set of relevant but different seen classes. We proposed a method that can learn discriminative features for ﬁne-grained category discovery by partitioning the data into k sub-datasets. We validated the effectiveness of our proposed method by setting a new state-of-the-artperformance on seven tested category discovery benchmarks.  Contrastive learning has beenexplored under this NCD problem by NCL [30], showing strong performance. We use k-means grouping on a self-supervised feature to provide infor-                mative pairs for contrastive learning. Our work also builds on a newly proposed setting named Generalized Category Discov-                ery. Contrastive learning has been showing to be effective for learning representations. Our focus is to learn representations that can be used to discover novel ﬁne-grained categories. By partitioning the dataset into k sub-datasets using MixUp, we can create informative contrastive pairs. -means, examples within each sub-dataset will be similar so that the model will be forced to learn more discriminative features. Compared to previous GCD methods with contrastivelearning [26], our method shows clear performance improvements. We partition the dataset into K sub- datasets using k-Means clustering. We perform joint contrastive representation learning on each of the partitioned sub- datasets. In GCD, the training dataset contains two parts, a labeled dataset Dl and an unlabeled dataset Du. The goal of GCD is to learn a model to categorize the instances in Du by leveraging the information from Dl. It has been shown that self-supervised ViT features could be a good initialization for representation learning in GCD. In this section, we review the method proposed in Vaze et al. [26] for GCD, which consists of two parts, representation learning and class assignment. We propose a simple method that partitions the dataset into k sub-datasets by using k-means on self-supervised features. The supervised contrastive loss is deﬁned as de）1, where N(i) is the set of indices of images in the minibatch that have the same label yi with the anchor image i. The overall procedure is similar to the original k-means method [20] for class assignments. The key challenge in representation learning for ﬁne-grained GCD is that the representa-                tion is required to be sensitive to the detailed discriminative traits of different classes. We take advantage of the self-supervisedrepresentations that can roughly cluster the images according to the overall image statistics. Ture vi = f(xi) is extracted from each image xi. The vi extracted by DINO [1] is incapable of distinguishing between the ﬁne-grained classes since there is no supervision during training. It will provide a rough description of the image so that similar images will be clustered together. The whole D is clustered into K sub-datasets {D1,D2,···,DK} using k-means. We apply both supervised contrastive loss and self-supervised contrastive lost to ﬁne-tune the model. The overall loss we propose to learn features is the combination of twolosses deﬁned above. The loss from Vaze et al. [26] can be viewed as a coarse-grained version of the loss from Eq. (3) loss Lcoarse compared to our proposed Lﬁne. After the representation is learned, we run the semi-supervised k-means                algorithm to obtain the cluster assignments of each sample. We evaluate our method on both generic image classi ﬁcation datasets and ﬃne-grained datasets. plit the training data into a labeled and an unlabeled dataset. We employ the clustering accuracy (ACC) on the unlabelled set to mea-sure the performance. The evaluation metric is deﬁned as below:ACC = maxp(yu) P is the set of all permutations that can match the predicts. We use ViT-B-16 [6] as the backbone of our method. The batch size for the entire training dataset is set to 256. For the ImageNet dataset, all models are trained for 60 epochs. We implement the projection heads as three layer MLPs. We compare XCon with the state-of-the-art methods on both generic image classiﬁ-cation benchmarks and ﬁne-grained images. For a fair comparison with existing methods, we use the same.semi-supervised k-means method as [26] to do the evaluation. The results are shown in Table 2. We present the results on ﬁne-grained image classiﬁcation benchmarks in Table 3. The best performance on ImageNet-100 ‘New’ subset is achieved by naively running a k-means on DINO features, suggesting that the original features can already represent the unlabeled categories well. Table 2: Results on generic datasets. CIFAR10, 100, 200 and Standford Cars, and train the model for 100 epochs to ablate the performance. Y. FEI ET AL.: LEARNING WITH EXPERTS FOR FINE-GRAINED CATEGORY DISCOVERY. The ACC is improved by 3.3-4.0% on CUB-200 and 15.4-28.5% on Standford Cars. XCon can consistently outperform the baseline(α = 0) with                different α, showing the robust effectiveness of our method. We analyze the choice of the weight α for ﬁne-grained loss in Table 5. XCon is consistently better than the baseline’s, but it still varies greatly depending on the number of sub-datasets. When K = 2, it can reach the high-                est on the ‘Old’ set, but the lowest on the New. The overall difference between features is not so great inside each group. features into two dimensions for a more direct qualitative analysis. In contrast to DINO, with our model, we can see clearboundaries between different groups, and each group is corresponding to one certain cate-                gory in CIFAR10.5. We propose XCon to address the problem of generalized category discovery with a self-supervised representation. model to learn ﬁne-grained discriminative features that can help discover categories. Experiments on four image classiﬁcation benchmarks show clear performance improvements of XCon, validating the effectiveness of our method. The author would like to acknowledge compute support from LunarAI. An empirical study of training self-supervised vision transformers. A simpleframework for contrastive learning of visual representations. A large-scale hierarchical image database. A new way to learn to discover novel classes given very limited data. A better way to teach people to use computer vision. -vain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXIV:2010.11929, 2020. The IEEE/CVF International.Conference on Computer Vision will be held in 2021. Masked autoencoders are scalable vision learners. Multi-class classiﬁcation without multi-class labels. Learning to cluster in order to transfer seamlessly across domains and tasks. Autonovel: Automatically discovering and learning novel visual categories. Masked Autoencoder: Scalable vision learners are scalable. Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip                Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Advances in Neural InformationProcessing Systems, 33:21798–21809, 2020. The Journa.sium on Mathematical Statistics and Probability, 1967. The. Journo’s. preprint arXiv:1306.5151, 2013. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2022. The California Institute of Technology created the birds-200-2011 dataset. The dataset was used to train a new type of machine-learning system. The results of the study were published in the journal of Machine Learning Research, 15(1):3221–3245, 2014. ning for novel class discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10867–10875, 2021. The self-supervised representation provides us the information to partition the training data into k expert sub-datasets, so we analyze the performance of our method by ﬁne-tuning                different pretrained representations. Supervised ViT models, i.e. MoCo v3 [3] and MAE [12], were used. We observe that DINO still shows the best performance on clustering the data based on class-irrelevant informations. The self-supervised representation of DINO [1] performs 7.3 −20.7% better than the other two. Our method tends to show better performance on ﬁne-grained datasets. We report the results on the Standfor-Cars and FGVC-Aircraft. dataset. We compare our estimated.number of classes in unlabeled data with the ground truth number of classes. in unlabelled data. With our estimated class number, our method performs better on Standford-Cars and also reaches comparable results on the other datasets except CIFAR10. We further ablate the components of contrastive loss in Table 11. We find that only with the combination of supervised contrastive. and unsupervised contrastive, i.e. λ = 0, the ACC drops 21.5 −23.6% on CUB-200 and 22.2 −46.6%. The method is also promising under the more realistic condition. s with our estimated class number. Unsupervised contrastive loss with the balanced parmeter. λ = 0.35 is necessary and can reach the best performance. CuCUB-200                Stanford-Cars                FGVC-Aircraft                Oxford-Pet.