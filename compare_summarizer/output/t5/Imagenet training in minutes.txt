finishing a 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU takes 14 days. this training requires 1018 single precision operations in total. on the other hand, the world’s current fastest supercomputer can finish 2  1017 single precision operations per second. if we can make full use of the computing capability of the fastest supercomputer for DNN training, we should be able to fin- ish the 90-epoch ResNet-50 in five seconds.... ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ the best results on scaling ImageNet training have used synchronous stochastic gradient descent (syncronous SGD) synchronous SGD has many inherent advan- tages, but at the root of these advantages is sequential consis- tency. this property is invaluable during DNN design and during the debugging of optimization algorithms. in particular, we scaled the training of GoogleNet to 128 Nvidia K20 GPUs with a batch size of 1K for 72 epoch ... ­­­­­­­­­­­­­­­­­­­­­­­­­­­ ­­­ ­­­ ­­­ ­­­­ ­­­ ­­­ ­ ­­­ each worker sends its local gradient wj to the master, and the master up- dates its weights by  w  w /P PP i=1 wj. for the second part, the master broadcasts  w to all work- ers. this synchronized approach is a widely-used method on large-scale systems. a more radical approach to breaking this synchronization barrier is to pursue a purely asynchronous approach.          ........... ­. ­ ­­ ­­­ ­­­­­­ ­­ Intel Knights Landing (KNL) is the latest version of In-tel’s general-purpose accelerator. the major distinct features of KNL that can benefit deep learning applications include the following: (1) self-hosted platform. KNL’s measured bandwidth is much higher than that of a 24-core Haswell CPU (450 GB/s vs 100 GB/s) since its release, KNL has been used in some HPC (high performance computing) data centers. since its release, KNL has............            . using a large batch size for SGD, the work for each iteration can be easily distributed to multiple processors. if we run 90 epochs, the number of operations is 90 * 1.28 Million * 7.72 Billion (1018). larger batch size will make the single GPU’s speed higher (Figure 3), which is why low-level matrix computation libraries will be more efficient. for large-batch training, we need to ensure that the larger batches achieve similar test accuracy with the smaller batches.... ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ if we increase the batch size from B to kB, the num- ber of iterations is reduced from I to I/k. this means that the frequency of weight updating reduced by k times. state- of-the-art approaches can only scale batch size to 1024 for AlexNet and 8192 for ResNet-50. if we increase the batch size to 4096 for AlexNet, we only achieve 53.1% in 100 epochs. our target is to achieve 58%   . ........... ­­. ­ ­­ ­­ ­­ ­­­­ ­­ ­­ ­­ ­ ­ we finish the 100-epoch training in 24 minutes. when we use 1024 CPUs, with a batch size per CPU of 32, we finish 100-epoch AlexNet training in 11 minutes. to the best of our knowl- edge, this is currently the fastest 100-epoch ImageNet train- ing with AlexNet. we finish the 90-epoch training in 32 minutes on 1600 CPUs. if they conduct 90-epoch training, the time is 80 minutes with... ­­ ­­­­­­­­­­­­­­­­­­­­­­­ ­­ ­­­ ­­ ­ ­­ ­­ ­­ ­ ­­ ­ ­ ­ ­ the baseline’s accuracy is lower than Facebook’s version, but we achieve a correspondingly higher ac- curacy when we increase the batch size above 10K. both Akiba et al. and Facebook’s work (Goyal et al. 2017) are ResNet-50 specific, while we also show the generality of our approach with AlexNet. since deep learning applications mainly use single-precision opera- tions, we do not consider double-precision here. ­­­­­­­­­­­­­­­­­­­­­­­­­­­­ ­­ ­­ ­ ­­­ ­­­­ ­­­­ ­­­ ­­­ ­­­ ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ the larger batch version sends fewer messages (latency overhead) and moves less data (bandwidth over- head) the larger batch size can increase the computation-communication ratio because it reduces the communication overhead (reduce latency and move less data) the most successful results on use the batch size equal to 32768, we finished the 90-epoch ImageNet training in 20 minutes without losing accuracy. the imagenet 1K benchmark set has played a significant role as a benchmark for assessing different ap- ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ researchers at facebook were able to scale the training of ResNet 50 to 256 Nvidia P100’s with a batch size of 8K and a total training time of one hour. using a more sophisticated approach to adapting the learning rate in a method they named the Layer-wise Adaptive Rate Scal- ing (LARS) algorithm. to achieve 58% accuracy, the larger batch (batch size = 4096) only needs about two hours while the smaller batch (batch size = 512 - gra­ ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ industrial sponsors include Intel, Google, HP, Huawei, LGE, Nokia, NVIDIA, Oracle and Samsung. in addition to ASPIRE sponsors, KK is supported by an auxiliary Deep Learning ISRA from Intel. XSEDE and Nvidia also thank XSEDE and Nvidia for independent support. XSEDE and Nvidia also thank XSEDE and Nvidia for independent support. XSEDE and Nvidia also thank XSEDE for independent support.                    - - - -. -. -..... -