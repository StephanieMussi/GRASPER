Decide collects version information of the components in five DL stack layers. if the local environment is a native Python environment, Decide uses pip. if the local environment is a vir- tual environment managed by Anaconda, Decide will use “conda list” to gather version information. if the local environment is a native Python environment, Decide will invoke “python --version” and parse the outputs. if the local environment is- n- s- n Decide detects and reports version issues within 1 minute without using GPU. on average, these projects have 1,647 stars and 1,321 lines of code. 13 issues involve components between two different layers, e.g., incompatibility be- tween TensorFlow and CUDA. watchman extracts dependency relations between third- party Python packages from PyEGo documentation. if a component is not compatible with all other components in the local stack, Decide reports no solution. .................................. PyEGo extracts dependency relations between Python pack- ages, Python interpreters, and system libraries based on the source code. in our benchmark, a majority of version issues involve the driver, OS/Container, and hardware layers. the success of Decide can be attributed to two factors: first, Decide is capable of detecting version issues across all five layers in a DL stack, while PyEGo and Watchman can de- tect version issues in at most two layers (i.e.. ­­­­­­­­­­­­­­­­­­­­­ ­­­­­­­­­­­­­­­­­ ­­­­ ­­­ ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ sample size is statistically significant with a 95% confidence interval. unifiedQA achieved 84.2% precision and 91.3% recall on these 360 queries. 12% of the incorrect predictions were because UnifiedQA misunderstood the question. for 53% of the incorrect predictions, the two DL components were just mentioned in the given paragraph and did not have dependency relationships. 35% of the incorrect predictions were due to a mismatch of components and their versions in the query. combining mul- ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ the majority vote strategy achieves the best ac- curacy, 95.1%, followed by voting by loss (93.6%) and weighted ma- jority vote (91.6%) our experiment results demonstrate the effectiveness of leverag- ing the rich version knowledge shared on Stack overflow to de- tect version incompatibility issues in deep learning. our pipeline for generating a large-scale and high-quality knowledge graph, which is easily extensible with more SO posts. however, ChatGPT... ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ in the future, we plan to investigate effective repair strategies based on the knowledge graph. in the future, we plan to fine-tune UnifiedQA by creating a large set of SO posts labeled version compatibility relationships. we believe it is a promising direction to improve Decide performance by substi- tuting UnifiedQA with stronger LLMs, such as ChatGPT, to extract version knowledge from SO posts. however, these techniques cannot be applied to Python-based DL projects due to the language difference   .. ­­­­­­­­­­­­­­­­­­­­­ ­­ ­­­­­­­­ ­­­­ ­­­ ­­­­­­­­­­­­­­­­­­­­­­­ ­­ ­­ ­ paper presents a knowledge-based version compatibility detection approach for deep learning projects. it uses a pre-trained Question- Answering (QA) model to extract version compatibility knowledge. it can accurately identify 65% of known version is- sues with a 92% precision. the authors would like to thank the anonymous reviewers for their valuable comments. ACKNOWLEDGMENT This research was in part supported by an Amazon Research Award and a Cisco Research Award; a Stack Overflow username :. arXiv preprint arXiv:1910.11015 http://arxiv.org/abs/1910.11015 http://stackoverflow.com/questions/59659585 Ryosuke Furuta, Naoto Inoue, and Toshihiko Yamasaki. 2019. An Empirical Study of the Dependency Networks of Deep Learning Li- braries. In 2020 IEEE International Conference on Software Maintenance and Evo- lution (ICSME). 868– ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering. suchita Mukherjee, Abigail Almanza, and Cindy Rubio-González. 2021. Fixing dependency errors for Python build reproducibility. https://archive.org/details/stackexchange Jibesh Patra, Pooja N. Dixit, and Michael Pradel. 2018. ConflictJS: Finding and Understanding Conflicts between JavaScript Libraries. - ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ ­­ ­­ ­­ ­ ­­ ­ ­ ­ ­­­­­­­­­­­­ ­­­ ­­­­­­­­ 68 Ying Wang, Ming Wen, Liu Zhenwei, Rongxin wu, Rui Wang, Bo Yang, Hai Yu, Zhiliang Zhu, and Shing-Chi Cheung. 2023. Knowledge-based Environment Dependency Inference for Python Programs. 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE). 1245–1256. Xiang Wei, Xingyu Cui, Ning Cheng,