the goal of this paper is to investigate and develop optimization techniques to accelerate training large deep neural networks. by increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes. by increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes. published as a conference paper at ICLR 2020 training Goyal et al. (2017). et al. (2017) was able to drastically reduce the training time of ResNet-50 model from 29 hours to 1 hour using a batch size of 8192. et al. (2017) developed a new optimization algorithm (LAMB) for achieving adaptivity of learning rate in SGD. ying et al. (2018) was able to train RESNET-50 on ImageNet in 2.2 minutes. empirically, none of these performance gains hold in other tasks such as BERT training. we assume function l(x) is Li-smooth with respect to x(i), i.e., there exists a constant Li. in this section, we discuss a general strategy to adapt the learning rate in large batch settings. a normalization of this form essentially ignores the size of the gradient. this is particularly useful in large batch settings where the direction of the gradient is largely preserved. in the next section, we discuss two specific algorithms to circumvent this issue. a simple function of (z) = minmaxz, l, u works well. the overall change in the learning rate is x(i) t  g(i) t . this can also be interpreted as an estimate on the inverse of Lipschitz constant of the gradient. the following result provides convergence rate for LAMB in general nonconvex settings. the convergence rate of LARS and LAMB is better than that of SGD. the results are presented as a conference paper at ICLR 2020 4 EXPERIMENTS. the results are compared with existing optimizers on two important training tasks. a TPUv3 Pod can provide more than 100 petaflops performance for mixed precision computing. we did not tune the hyperparameters of LAMB while increasing the batch size. with 32K batch size, we reduce BERT training time from 3 days to around 100 minutes. we consider the speedup is great because we use the synchronous data-parallelism. the baseline F1 score is the score obtained by the pre-trained model (BERT-Large) we use the same setting as the baseline: the first 9/10 of the total epochs used a sequence length of 128. the last 1/10 of the total epochs used a sequence length of 512. at a batch size of 32K, LAMB achieves 76.4% top-1 accuracy. at a batch size of 2K, LAMB is able to achieve 77.11% top-1 accuracy. at a batch size of 32K, LAMB achieves 76.4% top-1 accuracy while LARS achieves 76.3%. large batch techniques are critical to speeding up deep neural network training. in this paper, we propose the LAMB optimizer, which supports adaptive elementwise updating and layerwise learning. it is the first large batch adaptive solver that can achieve state-of-the-art accuracy on ImageNet training with RESNET-50. doi:10.1137/120880811. arXiv preprint arXiv:1705.08741, 2017. 9 Published as a conference paper at ICLR 2020 forrest n iandola,. Matthew W Moskewicz, Khalid Ashraf, and Kurt. Keutzer. One weird trick for parallelizing convolutional neural networks. imagenet training on imagenet in 74.7 seconds. arXiv preprint arXiv:1811.06992, 2018. Yang You, Igor Gitman, and Boris Ginsburg. Large-batch training for lstm and beyond. Published as a conference paper at ICLR 2020 APPENDIX A PROOF OF THEOREM 2 Proof. 2 t 2 u 2 L1 = f(xt) t h X i=1 (x(i) t ) if(xt)(i) t + if(xt)(i) t + if(xt)(i) t + if(xt) (i) t + if(xt) we analyze the convergence of LAMB for general minibatch size. x(i) t+1 = x(i) t t(x(i) t ) r(i) t r(i) t , for all i [h]. if 2 = 0, then T1 can be bounded as follows. t h i=1 di X j=1 r 1 2 G2di E h (x(i) t )  g(i) t,j i t h i=1 di X j=1 i,j  b. t h i=1 di X j=1 r 1 2 G2di E h N-LAMB and NN-LAMB can achieve a comparable accuracy compared to momentum solver. their performances are much better than momentum solver. if L 2 g and  2 g then LARS (i.e., gradient is more denser than curvature or stochasticity), we gain over SGD. results on several applications (Word2Vec, Image Recognition, and LSTM Language Model) showed that Nadam optimizer improves the speed of convergence and the quality of the learned models. we also tried using Nesterov’s momentum to replace the regular momentum of LAMB optimizer’s first moment. in this way, we got a new algorithm named as N-LAMB (Nesterov LAMB) the learning rate is the most important hyper-parameter that affects learning efficiency. a lower validation loss does not necessarily lead to a higher validation accuracy. a lower validation loss does not necessarily lead to a higher validation accuracy. a lower validation loss does not necessarily lead to a higher validation accuracy, a study shows. if the learning rate is tuned correctly, it will improve the learning efficiency. Figures 9 - 14 show the LAMB trust ratio at different iterations for ImageNet training with ResNet-50. there techniques help to im- prove the accuracy of Adam/AdamW/AdaGrad to around 73%. a lower validation loss may lead to a worse accuracy, so we use the test/val accuracy or F1 score on dev set to evaluate the optimizers. figure shows that LAMB can make the training converge smoothly at the extremely large batch size (e.g. 64K). figure 8: We achieve 76.8% scaling efficiency with a mixed, scaled batch size (65.2 times speedup by 64 times computational resources). Figure 12: The LAMB trust ratio. Figure 14: The LAMB trust ratio. we use the learning rate recipe of (Goyal et al., 2017) to tune Adam optimizer for ImageNet training with ResNet-50. the target accuracy is around 0.763 (Goyal et al., 2017). we use the 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. default (0.01) 0.0007954915 0.020 0.001 default (0.01) 0.0019124349 0.050 0.001 default (0.01) 0.0017089844 0.030 0.001 default (0.01) 0.0019124349 0.050 0.001 default (0.01) 0.018262305 0.018 0.001 default (0.01) 0.0017089844 0.030 0.001 default (0.01) 0.0019124349 0.050 0.001 default 0.00001 disable 0.6573079 0.010 0.00001 disable 0.65323895 0.018 0.00001 disable 0.65323895 0.020 0.00001 disable 0.66086835 0.025 0.00001 disable 0.6476237 0.040 0.00001 disable 0.55478925 0.050 0.00001 disable 0.61869305 29 Published as a conference paper at ICLR 2020 Table 18: The accuracy information of tuning default AdamW optimizer for ImageNet training with ResNet 0.016 0.01 disable 0.0041910806 0.020 0.01 disable 0.002746582 0.030 0.01 disable 0.0065104165 0.050 0.01 disable 0.007710775 32 Published as a conference paper at ICLR 2020 Table 21: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet- 50. we use the learning rate recipe of (Goyal et al., 2017) (1) 5-epoch warmup to stablize the initial stage. (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. the target accuracy is around 0.763 (Goyal et al., 2017) 0.014 0.00001 disable 0.7109375 0.016 0.00001 disable 0.7058309 0.018 0.00001 disable 0.7052409 0.020 0.00001 disable 0.7064412 0.025 0.00001 disable 0.7035319 0.030 0.00001 disable 0.6994629 0.040 0.00001 disable 0.6972656 0.050 0.00001 disable 0.6971232