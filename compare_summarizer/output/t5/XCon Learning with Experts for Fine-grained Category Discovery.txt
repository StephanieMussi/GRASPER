we present a novel method called Expert-Contrastive Learning (XCon) to help the model to learn fine-grained discriminative features. the method is based on a set of labeled images that contain both seen classes and unseen classes. the results show a clear improved performance over the previous best methods. the copyright of this document resides with its authors. it may be distributed unchanged freely in print or electronic form.        ­ ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ we propose a method that can learn discriminative features for fine-grained category discovery. we partitioned the data into k expert sub-datasets by performing k-means clustering. our code is available at https://github.com/YiXXin/XCon. a novel category discovery (NCD) problem can be solved using a self-supervised representation. if the method is successful, it could be used to improve the classification of unseen classes based on the goal of XCon is to learn representations that can be used to discover novel fine-grained categories within the unlabeled dataset. by creating informative contrastive pairs, examples within each sub-dataset will be similar so that the model will be forced to learn more discriminative features. compared to previous methods with contrastive learning [26], our method shows clear performance improvements. compared to the previous NCD problem that considers the class sets as Cl Cu = / 0, GCD is more challenging and ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ ­­ ­­­ learn- ing the model by contrasting between examples in the full dataset may not help the model to learn such a discriminative representation. each expert sub-dataset will be expected to reduce different class-irrelevant cues represented by different overall image statistics. we apply both supervised contrastive loss and self-supervised contrastive loss to fine-tune the model for fine-grained discriminative features. if the model is able to learn discriminative features, it will be able to learn discriminative features. the overall loss we propose to learn fine-grained features is the combination of two losses defined above Lfine = (1)Lu fine +Ll fine (6) Together with the loss from Vaze et al. [26], our optimization objective is L = Lcoarse +Lfine (7). after the representation is learned, we run the semi-supervised k-means algorithm to obtain the cluster assignments of each sample. we evaluate our method on both generic image classification datasets and fine-g .. ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ ­­­­ ­­ XCon shows the best performance on ‘All’, showing that our method could improve upon previous works. it also achieves comparable results with other methods on the other subsets as ‘Old’ and ‘New’, indicating the effectiveness of our method for fine-grained category discovery. for quicker evaluation, we use two fine-grained datasets, i.e. CUB-200 and Standford Cars, and train the model for 100 epochs to ablate the effectiveness of them.... ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ XCon can consistently outperform the baseline( = 0) with different, showing the robust effectiveness of our method. best result is achieved with  = 0.4 on CUB-200 and with  = 0.2 on Standford Cars. when K = 2, it can reach the high-est on the ‘Old’ set, but the lowest on the ‘New’ set. when K = 2, it can reach the lowest on the ‘New’ set, that means with two groups, the Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self- supervised vision transformers. Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, and Andrew Zisserman. Learning to discover novel visual categories via deep transfer clustering. in Proceedings of the IEEE/CVF International Conference on Computer Vision and Pattern, pages 8401–8409, 2022. Yen-Chang Hsu, Zhaoyang L. annay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Aaron Maschinot, Ce Liu, and Dilip Krishnan. supervised contrastive learning. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9462–9470, 2021. in Proceedings of the IEEE/CVF International Conference on Computer Vision and Pattern Recognition, pages 9462–9470, 2021. in Proceedings of the IEEE/CVF International Conference on Computer Vision and                   - - - - - ­­ Y. FEI ET AL. analyzes the performance of our method by fine-tuning different pretrained representations of other self-supervised ViT models. with the self-supervised representation of DINO, our method performs 7.3 20.7% better than the other two on CUB-200 and 1.8 22.4% better on Standford-Cars. our method tends to show better performance on fine-grained datasets, given that the dataset partitioning can help the model learn more................................. 1.7 82.4 loss and unsupervised contrastive loss with the balanced parmeter  = 0.35 is necessary and can reach the best performance.  CUB-200 Stanford-Cars All Old New All Old New 0 29.6 30.2 29.3 10.8 12.5 10.0 0.35 51.8 53.8 50.8 41.0 59.1 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 32.2 -       ........................