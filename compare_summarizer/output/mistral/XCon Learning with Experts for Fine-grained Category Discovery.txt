 This paper addresses the problem of generalized category discovery (GCD) in fine-grained datasets, where unlabeled images may contain both seen and unseen classes. The authors propose a method called Expert-Contrastive Learning (XCon) to help the model mine useful information from images by first  In this paper, Fei et al. propose a method called XCon for fine-grained category discovery on image classification benchmarks. The authors first partition a dataset into K sub-datasets using k-means clustering on a self-supervised representation. Each sub-dataset contains images with similar  The authors of this paper propose a method for fine-grained category discovery using self-supervised representation learning with expert sub-datasets. They analyze the performance of their method by fine-tuning different pretrained representations of other self-supervised ViT models, DINO, MoCo v3,  The text provides information on the performance of a model using two loss functions, known as cross-entropy loss and unsupervised contrastive loss, with a balanced parameter λ = 0.35, on various fine-grained datasets: CUB-200, Stanford-Cars, FGVC-Aircraft, Oxford-Pet, Old, New, Old, New, Old, New, Old, and New. The results are presented in Table 10, with the estimated class number for each dataset indicated. The text also includes an ablation study of contrastive loss, which is shown in Table 11. The study explores different values of the parameter λ and reports the corresponding performance on various datasets. According to the text, using cross-entropy loss and unsupervised contrastive loss with the balanced parameter λ = 0.35 is necessary for achieving the best performance. The ablation study in Table 11 demonstrates that this combination of losses yields better results than using only cross-entropy loss for most datasets, particularly for CUB-200 and Stanford-Cars. Specifically, the table shows that when λ = 0.35, the performance is significantly improved compared to when λ = 0 for all datasets except for a few cases where the performance is slightly lower than when λ = 0. Overall, the text emphasizes the importance of using both cross-entropy loss and unsupervised contrastive loss with an appropriate balance between them for fine-grained object recognition tasks.