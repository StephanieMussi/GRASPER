 This paper aims to investigate and develop optimization techniques for training large deep neural networks, focusing on approaches based on variants of Stochastic Gradient Descent (SGD). The authors note that while SGD iteratively updates model parameters by moving them in the negative direction of the gradient calculated on a minibatch  This paper discusses two optimization algorithms, LARS and LAMB, which are instantiations of a general strategy for large-batch training in deep learning. The strategy includes layerwise unit l2-norm normalization and scaling the learning rate by a function Ï†. LARS uses momentum optimizer  This paper proposes LAMB (Large-batch Adaptive Moments by Backpropagation), an optimizer for deep neural networks that supports adaptive elementwise updating and layerwise learning rates. The authors claim that LAMB is a general-purpose optimizer that performs better than standard SGD in  This text discusses various research papers related to deep learning, specifically focusing on techniques for improving training efficiency and convergence rates. The papers include "Deep residual learning for image recognition" by He et al., which introduces residual networks; "Train longer, generalize better" by Hoffer et al., which  This text discusses two optimization algorithms, N-LAMB and NN-LAMB, which are variants of the Large Batch Adaptive Moment Estimation (LAMB) algorithm. The authors compare these algorithms to SGD and LARS in terms of convergence rates. They also  The text discusses the performance of various optimizers, including LAMB, for training deep learning models on small datasets like CIFAR-10 and MNIST using DavidNet, as well as large datasets like ImageNet using ResNet-50. According to the results from DAWNBench,  The given text includes three tables from conference papers published at ICLR 2020, which report the accuracy information of training ResNet-50 on ImageNet using different optimizers (AdaGrad, default Adam, and default AdamW) with various learning rates and weight decays. The batch size  This text contains three tables from a conference paper published at ICLR 2020 that explore the impact of different learning rates, weight decays, L2 regularization, and other hyperparameters on the accuracy of ImageNet training using ResNet-50 with AdamW optimizer. The tables show the  This text provides accuracy information for training ImageNet with ResNet-50 using the AdamW optimizer, a batch size of 16384, and 90 epochs (7038 iterations). The learning rate recipe follows Goyal et al.'s method: a 5-epoch warmup to stabilize the initial stage, followed by multiplying the learning rate by 0.1 at the 30th, 60th, and 80th epochs. The target accuracy is around 0.763 (Goyal et al., 2017).

The tables present the learning rate, weight decay, L2 regularization, and Top-1 Validation Accuracy for each epoch. The first table shows values with weight decay set to the default 0.01 and L2 regularization disabled. The second and third tables have weight decay disabled but differ in their L2 regularization settings.

The learning rates range from 0.0001 to 0.01, while weight decay and L2 regularization values are either the default or disabled. Top-1 Validation Accuracy varies between 0.31193033 and 0.73065186. The highest accuracy is achieved with a learning rate of 0.004, weight decay disabled, and L2 regularization set to 0.0002.

These experiments were published as conference papers at ICLR 2020.