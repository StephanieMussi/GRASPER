 This paper explores scaling deep neural network (DNN) training on ImageNet using Large Batch Size Training with the LARS algorithm on Intel Knights Landing (KNL). The text begins by discussing the challenge of long training times for DNNs due to larger datasets and models. It highlights the potential  This paper compares two supercomputers, NERSC's Cori Phase 2 with 9,668 KNLs (Knights Landing processors) and TACC's Stampede 2 with 4,200 KNLs. The text discusses the  This text discusses the benefits of using large batches in deep learning, specifically for training image recognition models on the ImageNet dataset. The authors note that communication, which involves moving data between different levels of memory or over a network, is a major overhead when scaling algorithms on many processors. They illustrate this idea  The text discusses the benefits of using larger batch sizes in deep learning training algorithms, which can significantly reduce the number of iterations required. This is because each epoch only requires sending a fixed amount of data per message, proportional to the neural network model size |W|. With larger batch sizes n and epochs E, fewer messages are needed to complete the same number of floating point operations. The authors, I. Gitman and B. Ginsburg, developed this large batch training algorithm during Yang You's internship at NVIDIA in 2017. This research was supported by various grants and awards from organizations such as the National Science Foundation, U.S. Department of Energy, DARPA, and industrial sponsors including Intel, Google, HP, Huawei, LGE, Nokia, NVIDIA, Oracle, Samsung, Mathworks, and Cray. KK was also supported by an auxiliary Deep Learning ISRA from Intel, and CJH received independent support from XSEDE and Nvidia.