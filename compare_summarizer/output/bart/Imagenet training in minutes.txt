The ImageNet-1k benchmark set has served as a benchmark for assessing different approaches to DNN training. The LARS algorithm increased batch size to 32K for some DNN models. State-of-the-art ImageNet training speed with ResNet-50 is 74.9% top-1 test accuracy in 15 minutes. At very large batch sizes our accuracy is muchhigher than Facebook’s accuracy. Our accuracy scaling efﬁciency is.much higher than Facebook's version. of the algorithm (Goyal et al. 2017). Our code is available upon request (see the bottom of the page for more details) and we are happy to share our code with anyone who would like to use it for their own research or development. We are currently working on a version of LARS that can be used to scale the number of processors used in DNN. training and, and as a result, further reduce the total train-ridden time. The current fastest supercomputer can ﬁnish 2×1017 sin-                gle precision operations per second (Dongarra et al. 2017). If we can make full use of the computing capability of a supercomputer for DNN training, we should be able to do the 90-epoch ResNet-50 training in seconds. So far, the best results on scaling ImageNet training have usedsynchronous stochastic gradient descent (syncronous SGD). We use Layer-wise Adaptive Rate Scaling (LARS) to scale the batch size to very large sizes, such as 32K, although only 8 Nvidia P100 GPUs were employed. We found that us-gling LARS we could scale DNNTraining on ImageNet to 1070 CPUs and  58.6% accuracy. We could scale to 2048 KNLs and    without accuracy. State-of-the-art ImageNet is 74.9% top-art speed withResNet-1 test accuracy in 15 minutes (Akiba, Suzuki, Fukuda and Fukuda 2017) Data-Parallelism SGD is a widely-used method on large-scale systems. In synchronized data parallelism, the communication includes two parts: sum of local gradients and broadcast of global weight. Figure 2-(a) is an example of 4 worker machines and 1 master machine. The idea of a parameter server was used in commercial applications by the Downpour SGDapproach (Dean et al. 2012) However, Downpour’s performance on 1,600 GPUs for a globally connected network is not signiﬁcantly better than a single GPU (Seide et. al. 2014b) Model parallelism replicates the neu-repreral network itself on each machine. Partitioning the neural network means parallelizing the matrix opera-riddentions on the partitioned network. Thus, model parallelism can get the same solution as the single-machine case. For example, using 4 machines to par-allelize a 5-layer DNN. Intel Knights Landing (KNL) is the latest version of In-tel’s general-purpose accelerator. KNL does not need a CPU and is self-hosted by an operating system like CentOS 7. It is equipped with Multi-Channel DRAM (MCDRAM) and measured bandwidth is 475 GB/s. The data in this study is collected from training AlexNet by ImageNet dataset on NVIDIA M40 GPUs. Using 1024 CPUs, we can train AlexNet in 11 minutes and 90-epoch ResNet-50 in 48 minutes. Larger batch size needs much less iterations. The single iteration time can close to be much less than the total time to train the dataset. We have two chip options: (1) Intel Skylake CPU or (2) Intel KNL. Using 512 KNLs, we ﬁnish the 100-EpochAlexNet in 24 minutes. Data-parallelism is more stable for very large DNN training. By using a large batch size, the work for each iteration can be easily distributed to multiple processors. In a certain range, larger batch size will make the single GPU’s speed higher. For ImageNet training with Alexthe Net model the,ophobicoptimal batch size per GPU is 512. If we want to use many GPUs and make each GPU efﬁcient, we need a larger batchsize. For example, if we have 16 GPUs, then we should set the batch size to 16 × 512 = 8192. This means scaling ResNet50 iseasier than scaling AlexNet. Larger batch size means less number of iterations and less overall communication, which can improve the algorithm’S scalability. However, large batch does not change the number of ﬂoating point operations (computation vol-ume) For large-batch training, we need to ensure that the larger batches achieve similar test accuracy with the smaller batches by running the same number of epochs. State-of-the-art ap-proaches for large batch training include two techniques: linear scaling and warmup scheme. LARS algorithm (You, Gitman, and                Ginsburg 2017) together with warm up scheme (Goyal et al. 2017) can scale up the batch size. Using these two approaches, synchronous SGD with a large batch size can achieve the same accuracy as the baseline. Our target is to achieve 58% accuracy even when using large batch sizes. For versions without data augmentation, weachieve state-of the-art accuracy (73% in 90s) With augmentation our accuracy is 75.3%. We wrote our code based on Caffe (Grojia et. al. 2014) for single-machine processing. LARS algorithm is opened by NVIDIA Caffe 0.16. We run 100-epoch ImageNet training with AlexNet with standard single-precision. It takes 6 hours 9 minutes forbatch size = 512 on one NVIDIA DGX-1 station. After scaling the batch size to 32K, we are able to more KNLs. We use 512 KNL chips and thebatch size per KNL is 64. When we use 1024CPUs, with a batch size per CPU of 32, we ﬁnish 100- Epoch training in 11 minutes. To the best of our knowl-edge, this is currently the fastest 100-Epoch imageNet train-ing with Alex net. We are not afﬁliated to Intel or NVIDIA, and we do not have any a priori preference for KNL or P100. The overall comparison is in Table 9. The existing method does not work for Batch Size larger than 8K. LARS algorithm can help the large-batch to achieve the same accuracy with the same number of epochs. Both Facebook and Codreanu used data augmentation, Face- Glyphbook’s 90-epoch accuracy is higher than that of Codreuans’. We ob- Carbuncleserve that our scaling efﬁciency is much higher than Face-glyphbook's version. We use 512 KNLs to match Facebook’S 256 P100 GPUs. We used 512 KN lns to match the 256 256-pthread P100 of Intel KNL. The peak performance of a P100 GPU is 10.6 Tﬂops5, and the peak perfor-mance of an Intel KN ln is 6 T ﬁops6. According to linear scaling, the optimal learning rate (momentum) of 4096 should be 2.9 and weight decay is 0.9. In recent years the ImageNet 1K benchmark set has played a signi�cant role as a benchmark for assessing different apaches to DNN training. The most successful results on ImageNet have been based on the batch size rule. The larger batch size makes the larger batch- algorithm more scalable on more distributed systems. The large batch version sends fewer messages(latency overhead) and moves less data (bandwidth over-head) than the small batch version. The largest batch version needs to move much less data than smaller batchversion when they ﬁnish the number of ﬂoating point op- vantageerations (Figures 9 and 10). In summary, the larger batches size does not change the amount of computation-communication overhead when we increase the size of the dataset. It does, however, make the training process more efficient and less expensive to run on a large number of processors. Researchers at Facebook (Goyal et al. 2017) were able to scale the training of ResNet 50 to 256 Nvidia P100’s with a batch size of 8K and a total training time of one hour. Large batch can achieve the same accuracy in the ﬁxed number of ﬂoating point op-676erations. The large batch (size = 512) needs about six hours while the smaller batch (batch needs about two hours) only needs about four hours. When we have enough computational powers, thelarger batch is much faster than the smaller version. When the number of epochs is increased, the larger batch is faster than when we have a smaller batch. To scale this synchronous SGD.approach to more processors requires increasing the batch.size. Using a more sophisticated approach to adapting the learning rate. in a method they named the Layer-wise Adaptive Rate Scal-ing (LARS) algorithm (You, Gitman, and Ginsman, 2017), we can scale the batch size to 32K; how-ever scaling to larger. numbers was not demonstrated in that work. The large batch training algorithm was developed jointly with I.Gitman and B.Ginsburg at NVIDIA in the summer 2017. The larger batch version of the algorithm needs to move much less data than the smaller batch when the number of operations is increased. The work was supported by the National Sci-riddenence Foundation, through the Stampede 2 (OAC-1540931) Award, and by the U.S. DOE of Science, of Advanced Scientiﬁc Computing Research, Applied Mathematics program under Award DE-SC0010200; by the DARPA Award HR0011-12- 2-0016; and by an auxiliary Deep Learning ISRA from Intel. CJH also thank XSEDE and Nvidia for independent supportivesupport. For more information, visit NVIDIA’s Deep Learning blog and the NVIDIA Deep Learning website. For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit www.suicidepreventionlifeline.org.