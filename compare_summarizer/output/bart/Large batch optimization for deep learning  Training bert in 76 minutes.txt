Published as a conference paper at ICLR 2020. Training large deep neural networks on massive datasets is computationally verychallenging. There has been recent surge in interest in using large batch stochasticoptimization methods to tackle this issue. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes. Using these tricks, Goyal et. al. (2017) was able to drastically reduce the training time of ResNet-50. from 29 hours to 1 hour using a batch size of 8192. These works demonstrate the need for an adaptive learning rate mechanism for large batch learning. We develop a new optimization algorithm (LAMB) forachieving adaptivity of learning rate in SGD. Adaptive learning rates are used to reduce hand-tuning of hyperparameters. The fastest training result for RESNET-50 on ImageNet is due to Ying et al. (2018), who achieve 76+% top-1 accuracy. By using the LARS optimizer and scaling the batch size to 32K on a TPUv3 Pod, Ying and his colleagues were able to train RESnet-50 in 2.2 minutes. However, none of these performance gains hold in other tasks such as BERT training. In this section, we discuss a general strategy to adapt the learning rate in large batch settings. Since ourprimary focus is on deep learning, our discussion is centered around training a h-layer neural network. We propose the following two changes to the update rule: The update is normalized to unit l2-norm. The learning rate is scaled by φ(∥xt∥) for some function φ : R+ →R+. The general strategy is LARS algorithm (You et al., 2017), which is obtained by using momentum optimizer as the base algorithm A in the framework. LARS was earlier proposed for large batch learning for RESNET on ImageNet. We now provide convergence analysis for LARS in general nonconvex setting stated in this paper. We will defer all discussions about the convergence Rate to the end of the section. ADAM optimizer is popular in deep learning community and has shown to have good performance for training state-of-the-art language models like BERT. Unlike LARS, the adaptivity of LAMB is two-fold: per dimension normalization with respect to the square root of the second moment used in ADAM and layerwise adaptivity. When β1 = 0 and β2 = 0, the algorithm reduces to Sign SGD. We use a polynomially decaying learning rate of ηt = η0×(1−t/T) ingorithm 2. We run with the same number of epochs as the baseline but with batchsize scaled from 512 to 32K. With 32K batch size, we reduce BERT training time from 3 days to around 100 minutes. We achieved 49.1 times speedup by 64 times computational resources. We use the F1 score on SQuAD-v1 as the accuracy metric. We use TPUv3s in our experiments. All the experiments run the same number of epochs. It is worth noting that we can achieve better results by manually tuning the hyperparameters. We conclude that ADAMW does not work well in large-batch BERT training or is at least hard to tune. The baseline can get 76.3% top-1 accuracy in 90 epochs (Goyal et al., 2017) All the successfulplementations are based on momentum SGD or LARS optimizer. We did not tune the hyperparameters of LAMB optimizer when increasing the batch size. We use the square root scaling rule and linear-epoch warmup scheduling to automatically adjust learning rate. We use square root LR scaling and linear-epoch warmup. The baseline Goyal et al. (2017) gets 76.3%top-1 accuracy in 90 epochs. LAMB can achieve an even higher accuracy if we tune the hyperparameters. We are able to scale the batch size of BERT pre-training to 64K without losing accuracy. This reduces the BERT training time from 3 days to around 76minutes. Researchers have developed a way to train deep neural networks in a shorter time frame. They have also developed a method for solving the convex convex programming problem with convergence rate of 2.1/k volume. The research was published in the open-access journal arXiv. Back to Mail Online home.Back to the page you came from. We analyze the convergence of LARS for general minibatch size. We show how the update of the LARS function f is the following:f(xt+1) + ⟨∇if(xt), x(i) (−1) - g (i) - h (i), i (h) - f(xt) + (x(i), f (xt), g (xt) - (x (i, i), i) -f (xi, i, i). The inequality follows from the lipschitz continuous nature of the gradient. Let the above inequality can be rewritten in the following manner:                f(xt+1) ≤ f(xt) −ηt  Hierarchy of inequality in the inequality, using Cauchy-Schwarzz inequality. Using we-warz inequality, the inequality above, we have: (1+f(1) + (2) (3)  (4) We analyze the convergence of LAMB for general minibatch size here. For simplicity of notation, we reason the parameter f is L-smooth. Recall that the update of L AMB is the following:   ‘‘ ’’, ‘.’ ‘”, “’. ’”. “.” ‘,’,. ’,”,. ‘;. ”.‘, ’. ,  ,.  , ,.’;.“, ”, “” The above inequality simply follows from the lipschitz continuous nature of the gradient. We boundterm T1 in the following manner: T1 leads to T1. If β2 = 0, then T1 can be bounded as follows: T1 Leads To T1, where T1 is a bound term. Substituting the above bound on T1 in equation 7, we have the following bound:                E[f(xt+1)] ≤f(xf(xt) −ηtαl                              ‘‘ ’’ ‘ ’: “”: “’.”  :   :””,  , ”, ,’,”;.  ,.  ; ,. ; ”.’;. . ”;” We use the learning rate recipe of Nadam to stablize the initial stage. We multiply the learning Rate by 0.1 at 30th, 60th, and 80th epochs. The target accuracy is around 0.763 (Goyal et al., 2017) We can also use Nesterov’s momentum optimizer to replace the regular momentum of LAMB. N-LAMB (Nesterov’s momentum for both the ﬁrst moment and the second moment) is better than momentum solver (Figure 1) LAMB is able to achieve 94.08% test accuracy in 24 epochs, which.is better than other adaptive optimizers and momentum SGD. Even on the smaller tasks like MNIST training with LeNet, LAMb is able. to achieve better accuracy than existing solvers (Table 7) The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is. {0.0001, 0.0002, 0,0004,                0.0006, 0,.0008,  0.001,  0.002,    0.006,  1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}. The momentum optimizer was tuned by the baseline. implementer. The weight decay term of AdamW was tuned. by 0.001. We use the test/val accuracy or F1 score on dev set to evaluate the optimizers. The target accuracy is around 76.3% (Goyal et al., 2017). There techniques help to im-prove the accuracy of Adam/AdamW/AdaGrad to around 73%. However, even with these techniques, even the best optimizers stil can not achieve the target validation accuracy. LAMB can make the training converge smoothly at the extremely large batch size (e.g. 64K) We achieve 76.8% scaling efﬁciency (49 times speedup by 64 times computational resources) We use the learning rate of Goyal et al. (2017) to AdamW and refer to it as AdamW+. The third tuning set is based on AdamW+ with default L2 regularization. Table 11: The accuracy information of tuning default Adam optimizer for ImageNet training with ResNet-50. We use the learning rate recipe of (Goyal et al., 2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning. rate by 0.1 at the 30th, 60th, and 80th epoch. The target accuracy is around 0.763. Table 14: The accuracy information of tuning default AdamW optimizer for ImageNet training. The target accuracy is around 0.763. (Goyal et. al., 2017).Table 15: The. accuracy information for tuning default. AdamWoptimizer with. ImageNet-50 (batch size = 16384, 90 epochs, 7038. iterations) Table 16: The accuracy information of tuning default AdamW optimizer for ImageNet training. The target accuracy is around 0.763 (Goyal et al., 2017) Table 17: The default optimizer optimizer tuning with AdamW training. AdamW. optimizer training with AdamResNet-50 (batch size = 16384, 90 epochs, 7038 iterations) Table 18: The accuracy information of tuning default AdamW optimizer for ImageNet training with the target accuracy is around 0.763. The target accuracy of the AdamWoptimizer is 0.7. The AdamW Optimizer is used to train the ImageNet image search engine. Table 19: The. accuracy information. of tuning the default. AdamWOptimizer for image search with the. target. accuracy of 0.765. Table 20: The accuracy information of tuning default AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 (Goyal et al., 2017). We use the learning rate recipe of Goyal (2017) (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rates by 0.1 at the 30th, 60th, and 80th epochs. Table 22: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-reprehensive50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al., 2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rates by 0.1 at the 60th, 60th and 80th epoch. The target accuracy is around 0.763 (G Loyal et al. 2017). We use the learning rate recipe of (Goyal et. al., 2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the. learning rate by 0.1 at the 30th, 60th, and 80th epochs. The target accuracy is around 0.763. We use the tuning AdamW optimizer for ImageNet training with ResNet-reprehensible. 0.0250.00001. Carbuncledisable0.7035319reprehensive0.030reprehensible0.6994629reprehensibly0.6971232reprehensively0.7052656reprehensivespeculative0.7004629preconceptionspeculativespeculatively 0.6972656preconceptionprognosively 0.6992629PreconceptionPrognosivespeconceptionspreconceptsPreconceptionsPreConceptionspreConceptionPreConcepts PreConceptions PreConception Preconceptions PostConception