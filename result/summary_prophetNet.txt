in this paper, we address the problem of generalized category discovery ( gcd ). 1. 1 xcon : introduction to category discovery we present a novel method called expert - contrastive learning to help the model to learn discriminative features from the images. 2. 1 in this case, we present the method of category discovery to the models to the model, where the model can be seen to be an example of a case where the models can be used to mine useful information from the datasets. 3. 1 it is difﬁcult for the model to assume that the collected annotations are accurate and the annotation is accurate, but also it is costly to collect because of the time and expense involved, and it is not possible to collect enough annotates to be accurate, and also it could be expensive to collect. this document resides with its authors, where the author is the author of this document, and the authors are the authors. the authors reside with their authors, and this document is distributed unchanged by the authors, as well as by their authors. from a set of labeled datasets. see the left part of fig. 1. we have ob - served this observation by observing that an unsupervised representation ( ) could group the data based on class y, see the right part of figure 1. see also : we have observed that an object pose such as the object pose and the background. see the bottom part of the data. see our left part. see this part of our data see. see that the data see this side of the table. see left part : see left of data see left side of fig 1. our proposed xcon method we proposed our proposed method, we proposed a method that can be used as a strong prior prior to the next learning phase. these k sub - datasets can be viewed as the starting point for the next dataset. this dataset can be seen as a good prior prior. this is a good starting point because each of these k data - sets will be able to be used to learn a robust representation. this means that each to categorize the unseen classes further. a new scenario is proposed under this ncd problem where the representation is used for contrastive learning. the representation has been shown to be effective under this problem. a common three - step learning pipeline is proposed to be used under this situation. this problem has been proposed under the unlabeled dataset, and then further developed using a pair - wise clustering loss using the pairwise clustering - loss on the labeled dataset and then is further refined using the two - step training pipeline. the representations are then further learned using the representation as the representation. this is the problem under which many works [ 4, 7, 9 ]. this work has shown promising results under this dataset. this data set has shown strong performance under this challenge. our work has been focused on this problem, showing strong performance. our focus is to explore this problem under this case, showing that this problem is a very challenging one. we focus on the ncd situation. our our method shows clear performance improvements. 4 methods in our method using our method with contrastive learning, our method has shown clear performance improvement. our methods show clear performance gains. 3 methods in gcd, the method of our method uses our method to learn more discriminative features. our method, using contrastive representation learning [ 26 ]. 5 methods in the ﬁne - grained, we ﬁrst partitioned the dataset using a partitioned dataset, then we perform joint contrastive representations learning on the full dataset for the gcd dataset, we proposed a method that uses k - means on features to help construct a classiﬁcation setting where the model needs to learn about categories. for representation learning, we propose to use the method proposed in the previous section of our framework, we presented a simple method that we proposed to use for the representation learning setting. for class learning setting, we review the method that is proposed in vaze et al. [ 26 ], for gcd, the method is proposed to be implemented in the following section, representation learning and class assignment : class assignment and representation learning : representation learning is proposed. for the class assignment, vaze [ 25 ] proposes to perform the representation by performing representation learning on the labeled data by performing in each step to recompute the assignment, the model is aware of the fact that the cluster assignment was made in dl regardless of the distance to the nearest cluster centroids regardless of its distance to dl, and thus, the sample will be assigned to the correct cluster regardless of how close it is to dl or dl. 3. 3 learning discrimina - tions to the full dataset may not help the model to learn to be sensitive to such a representation. thus, we take advantage of the self - supervised representations to perform a preprocess on each example within each of the images within each example, we use the model by performing a prepa - tion on the whole dataset, we perform a par - tition on each image, and then we use it to learn - ing the models to learn a representation to learna - ing different classes of different classes. then, we expect the model will naturally learn to differentiate between different class - irrelevant cues represented by different classes - irrelevant each projector can be consid - ered an expert projector dedicated to learning ﬁne - tuned features from each sub - dataset. we apply both of these approaches to learn the model. we use both methods to learn a model. each modelor can also be dedicated to learn discriminant features. speci - erly, we apply the model to learn from the datasets. as a result, we propose to apply both supervised contrastive loss and self - supervised contrastion loss, respectively. we propose that the model should be modeled as follows : following previous works, we choose as the generic image classiﬁcation datasets as the image classesi -cations. we choose cifar - 10 / 100 as the images classi ﬁcations, and we choose to use cifar100 / 100 for the generic class classi classifier data, as the general classi - cessions. for example, we chose to use scars pet as the pet category. these categories contain categories from the same entry level classes, e. g., birds and pet. for pet we choose scars pet, and pet pet. these ﬁne - grained ﬁnes contain categories that contain the same categories. this can be challenging for models requiring models to learn highly discriminative features, including 4. 2 implementation details we follow the implementation of [ 26 ] to implement our method. we follow it to the letter to use as the backbone of our method to use. we use vit - b - 16 [ 6 ] to use it to ﬁne - tune the ﬁnal transformer block while cu \ cl is frozen while other blocks are frozen while others are frozen, other blocks will be frozen while testing. we only ﬁneﬁne the model while other models are frozen. we ﬁnd the model with the parameters pretrained by dino [ 1 ] on imagenet and only use the model when testing, we ﬁne the training datasets to be trained for 60 epochs. we set α to be 0. 1 by default. for our method, we use the following methods : we set the training parameters to be set to 0. 5 by default and we set our method can best preserve the original fea - tures, suggesting that unlike ex - isting method potentially introducing damage to original features which results in performance drop, our method is best preserving the original features of original categories well, showing that by using our method to preserve original features, whereas ex - ex isting methods can best protect original features and original features. our method shows the best performance on all four datasets we tested, we show that our method showed the best results on the all dataset, while achieving comparable results on ‘ all ’ dataset while achieving similar results on old dataset. we present the results on category discovery dataset i. e. cub - 8 y. we use the same dataset as cub - 9 y. for quicker evaluation, we perform the ablation study. 4. 3 evaluation - - - gcd [ 25 ] 51. 1 51. 0 52. 0 51. 3 52. 1 52. 2 51. 2 52. 3 51. 4 51. 5 51. 6 51. 7 - - 0 - - 4 - - 1 - - 2 - - 3 - - 5 - - 6 - - 9 - - 8 - -. 0 - 0. 0 0. 1 0. 2 - 0 0 - 1. 0 xcon and standford cars. we train the model for 100 epochs, and train it for 50 epochs. we observe that the performance of xcon is improved by using dif - ferent combinations of loss terms. we observed that with additional supervision from the coarse - grained loss term, the acc achieves the best performance. as using the coarse loss terms, the performance improves by 3. 3 - 4. 0 %. as the performance is achieved by using a coarse loss term terms, as the coarse losses term terms. as combining with one group, it can reach the high - est and the lowest - est. that means with two groups, the model tends to focus more on the features inside each group, that means when with three groups, it tends to see classes that are similar to the seen classes, that is with two classes, and with three classes, there is no difference between features. 0 0. 0 is the baseline. α cub - 200 stanford - cars all the time 0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 9 0. 8 0. 10 0. 11 0. 15 0. 16 0. 17 0. 18 0. 19 0. 20 0. 21 0. 12 0. 13 0. 14 0. 00 0. 01 0. 001 0. 000 0. 02 0. 04 0. 03 0. 05 0. 08 0. 07 0. 09 0. 06 0. 99 0. 98 0. acknowledge the author would like to acknowledge compute support from lunarai. acknowledge xcon has proven itself to be a success with xcon. acknowledging xcon shows clear performance improvements of xcon and xcon 2. 0. 10y. xcon 1. 0 acknowledge the authors would like the author to acknowledge that we have a method that works. k cub - 200 stanford - cars cub - 100 stanford - drivers cub - 2000 stanford - vehicles cub - 300 stanford - car. 1. 1 2. 4. 0 2. 2. 1 an image is worth 16x16 words : transformers for image recognition. in proceedings of the ieee international conference on computer vision and machine intelligence, 2010. [ 6 ] kai han, in proceedings at the ieee annual conference on learning representations, 2019. [ 7 ] in proceedings : the ieee / ieee conference on computing representations, 2021. [ 8 ] kai ha, and elisa ricci. a uniﬁed objective for novel class discovery. [ 9 ] kai he, sebastien verhoeven, and sebastien haussmann. an objective objective for new class discovery : a novel class. in conference proceedings of ieee / cvf : a unicyclic objective. [ 10 ] kai. a singular objective. a single objective. an object. [ 11 ] [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ] [ 18 ] [ 19 ] [ 20 ] [ 21 ] [ 22 ] [ [ ] ] [ ] [. ] [ 23 ] [ 24 ] [ a ] ]. [ 18. [ 19. [ [ 17. ] ] 2019. [ 20. [ ] 2019 ] [ 11 ] [ 25 ] [ 26 ] [ 27 ] [ 28 ] [ 29 ] [ 33 ] [ 34 ] [ 31 ] [ 35 ] [ edit ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] [ 10 ] [ 11 ] [ 9 ] [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 18 ] [ 17 ] [ ] ] [ 19 ] [ 20 ] [ [ ] [ 21 ] [ 22 ] [ 24 ] [ 25 ] [ 26 ] [ 28 ] [ 35 ] [ 34 ] [ 36 ] [ 33 ] [ 29 ] [ 30 ] [ 31 ] [ 32 ] [ edit ] open - mix : reviving known knowledge. in proceedings of the ieee / cvf conference on computer vision and pattern recognition. open - mixed : revisiting known knowledge by re - discovering known knowledge, we present the results of our research. [ 1 ] in proceedings. [ 2 ]. [ 3 ] rui zhu. improv - ing contrastive learning by visualizing feature transformation and feature transformation. [ 4 ] [ 5 ], [ 6 ], and [ 7 ], respectively. [ 8 ] [ 9 ] and [ 10 ]. we observe that our method performs well on the training data, i. e., we partition the dataset into k expert sub - datasets. we analyze the performance of our method by comparing the performance with other self - supervised models. we use the method to partition the trained dataset we ﬁnd that on standford - cars, our method is signiﬁcantly closer to the ground truth than on the other ﬁve datasets, the number of classes estimated by our method on standfor - cars is higher than on cub - 200, and on the standford cars, the method is also more accurate than the method of gcd, given that the dataset partitioning can help the model learn more discriminative features. our method tends to show better performance on the ﬁne - grained dataset, given the data set partitioning and the fact that the model is partitioned, the model tends to perform better on the more challenging dataset. c performance with estimated class number we report the results on generic image classiﬁcation benchmarks. b performance with estimate of class number cu we report on the results of our method. a performance with estimation of class numbers cu. c results with estimate class number our method performs better under the more realistic condition. table 10 : estimated class number. known cu cub - 200 stanford - cars cu cu - 200 cu - 300 cu - 100 stanford - car cu cub cu 200 cu cu 200 - cars all new all class number with our estimated class numbers. known class number : unknown class number unknown class numbers with our estimate class number of class number and class number is unknown. unknown cu cub cub - 300 stanford - computers all the time. the balanced parmeter λ = 0. 35 is necessary and can reach the best performance. 4 loss and loss of contrastive loss with the balanced - parmeter is necessary. 4