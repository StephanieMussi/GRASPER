{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMsrySAKzEq6"
   },
   "source": [
    "\\# Welcome to CS 5242 **Homework 4**\n",
    "\n",
    "ASSIGNMENT DEADLINE ‚è∞ : **23:59 04 April 2024**\n",
    "\n",
    "In this assignment, we will delve into **different generation methods of GPT**. To be specific, you will practice different text generation methods using the powerful [OPT](https://arxiv.org/abs/2205.01068) language model based on [transformers](https://huggingface.co/docs/transformers/en/index) package. You will implement two generation techniques: greedy search, beam search and sampling.\n",
    "\n",
    "Helpful material: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navdev2/.local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized input: tensor([[    2,  1121,    42, 11717,     6,    52,    40, 33244,    88,   430,\n",
      "          2706,  6448,     9,   272, 10311,     4,   598,    28,  2167,     6,\n",
      "            47,    40]]), shape: torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the OPT model and tokenizer\n",
    "# To save memory, we only use opt-350m\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "prompt = \"In this assignment, we will delve into different generation methods of GPT. To be specific, you will\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(f\"tokenized input: {input_ids}, shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye5nZ6OVI6qg"
   },
   "source": [
    "### Task 1:  Greedy Search\n",
    "\n",
    "Greedy search is a simple and straightforward method for text generation. At each step, it selects the next token with the highest probability according to the language model's output. This approach is called \"greedy\" because it greedily chooses the most likely token at each time step without considering future consequences. Greedy search is fast but may lead to repetitive or monotonous results.\n",
    "\n",
    "Greedy search is a simple method where at each time step, the word with the highest probability is chosen as the next predicted word, until an end token is generated or the maximum length is reached.\n",
    "\n",
    "It can be represented by the formula:\n",
    "\n",
    "$ w_t = \\arg\\max P(w_t | w_{<t}) $\n",
    "\n",
    "Here, $ w_t $ is the predicted word at time step $ t $, and $ P(w_t | w_{<t}) $ is the probability of predicting $ w_t $ given the previous word sequence $ w_{<t} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this assignment, we will delve into different generation methods of GPT. To be specific, you will\n",
      "generated_text: In this assignment, we will delve into different generation methods of GPT. To be specific, you will be working on a GPT-based system that is based on the following:\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n"
     ]
    }
   ],
   "source": [
    "def greedy_search_generation(input_ids, model) -> str:\n",
    "    outputs = input_ids\n",
    "    \n",
    "    # =========================\n",
    "    # Your code starts here (3 points)\n",
    "    # outputs should be a torch Tensor of shape [1, n]\n",
    "    # where n is the length of the generated text\n",
    "    # and the elements are the token ids of the generated text\n",
    "    # \n",
    "    # You should not use any external library like transformers's .generate().\n",
    "    # Only torch is allowed.\n",
    "    # =========================\n",
    "    max_length = 64 #max length as in announcement\n",
    "    with torch.no_grad():  \n",
    "        for _ in range(max_length):\n",
    "            logits = model(outputs).logits[:, -1, :]\n",
    "            max_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            outputs = torch.cat([outputs, max_id], dim=1)\n",
    "            \n",
    "            if max_id.item() == tokenizer.eos_token_id: #eos\n",
    "                break\n",
    "    \n",
    "    # =========================\n",
    "    # Your code ends here\n",
    "    # =========================\n",
    "\n",
    "    return outputs\n",
    "\n",
    "generated_ids = greedy_search_generation(input_ids, model)[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQQ3k-DBFGEb"
   },
   "source": [
    "### Task 2: Beam Search\n",
    "\n",
    "\n",
    "Beam search is a more complex method that considers multiple candidate words rather than just a single word. At each time step, it keeps track of the top $ k $ most likely partial sequences (known as the beam width), and then expands these partial sequences based on their scores. Finally, it selects the sequence with the highest score from these expanded sequences. Beam search can provide more diverse results but might still result in repetitive outputs.\n",
    "\n",
    "Here's the basic algorithm for beam search:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with an initial input sequence (usually a special token indicating the beginning of a sequence).\n",
    "   - Initialize a set of beams, each containing the initial sequence and its probability score.\n",
    "\n",
    "2. **Expansion**:\n",
    "   - For each beam, generate the next token in the sequence using the GPT model.\n",
    "   - Calculate the conditional probability of each possible next token given the current partial sequence.\n",
    "   - Expand each beam by appending each possible next token to the partial sequence, along with its updated probability score.\n",
    "\n",
    "3. **Selection**:\n",
    "   - Select the top-k beams with the highest probability scores, where k is the beam width.\n",
    "   - Discard the remaining beams.\n",
    "\n",
    "4. **Termination**:\n",
    "   - If the generated token is an end-of-sequence token or a maximum sequence length is reached, terminate those beams.\n",
    "   - Repeat steps 2-4 until all remaining beams either terminate or reach the maximum sequence length.\n",
    "\n",
    "The beam search algorithm can be expressed mathematically as follows:\n",
    "\n",
    "- Let $ S_t^b $ denote the partial sequence for beam $ b $ at time step $ t $.\n",
    "- Let $ P(S_t^b) $ denote the probability of the partial sequence $ S_t^b $ up to time step $ t $.\n",
    "- Let $ P(w_t | S_t^b) $ denote the conditional probability of the next token $ w_t $ given the partial sequence $ S_t^b $ at time step $ t $.\n",
    "- Let $ K $ denote the beam width.\n",
    "\n",
    "The beam search algorithm can be summarized with the following formulas:\n",
    "\n",
    "1. **Initialization**:\n",
    "   $ S_1^1 = \\text{[BOS]} $\n",
    "   $ P(S_1^1) = 1 $\n",
    "\n",
    "2. **Expansion**:\n",
    "   $ P(w_t | S_t^b) = \\text{GPT\\_Model}(S_t^b, w_t) $\n",
    "   $ S_{t+1}^b = S_t^b \\cup \\{w_t\\} $\n",
    "   $ P(S_{t+1}^b) = P(S_t^b) \\times P(w_t | S_t^b) $\n",
    "\n",
    "3. **Selection**:\n",
    "   $ (S_{t+1}^{(1)}, ..., S_{t+1}^{(K)}) = \\text{Top-K}(S_{t+1}^1, ..., S_{t+1}^B, K) $\n",
    "\n",
    "4. **Termination**:\n",
    "   - Terminate beams that reach the maximum length or encounter an end-of-sequence token.\n",
    "\n",
    "This process continues until all terminated or maximum-length beams are obtained.\n",
    "\n",
    "In summary, beam search efficiently explores the space of possible sequences and provides a trade-off between exploration and exploitation, helping to find high-quality sequences in sequence generation tasks such as text generation with models like GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this assignment, we will delve into different generation methods of GPT. To be specific, you will\n",
      "generated_text: In this assignment, we will delve into different generation methods of GPT. To be specific, you will be working on a GPT-based system that is based on the following:\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n"
     ]
    }
   ],
   "source": [
    "def beam_search_generation(input_ids, model, beam_size) -> str:\n",
    "    outputs = input_ids\n",
    "    \n",
    "    # =========================\n",
    "    # Your code starts here (4 points)\n",
    "    # outputs should be a torch Tensor of shape [1, n]\n",
    "    # where n is the length of the generated text\n",
    "    # and the elements are the token ids of the generated text\n",
    "    # \n",
    "    # You should not use any external library like transformers's .generate().\n",
    "    # Only torch is allowed.\n",
    "    # =========================\n",
    "    \n",
    "    beams = []\n",
    "    for i in range(beam_size):\n",
    "        beams.append(([input_id.item() for input_id in input_ids[0]], 0)) #init to 0 because log(1)=0\n",
    "    \n",
    "    max_length = 64 #max length as in announcement\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            candidates = []\n",
    "            for beam, prob in beams:\n",
    "                beam_input_ids = torch.tensor([beam])\n",
    "                if beam[-1] == tokenizer.eos_token_id: #eos, no need to process\n",
    "                    candidates.append((beam, prob))\n",
    "                    continue\n",
    "                \n",
    "                logits = model(beam_input_ids).logits[0, -1, :]  \n",
    "                log_probs = torch.log_softmax(logits, dim=-1) \n",
    "                \n",
    "                for token, log_prob in enumerate(log_probs):\n",
    "                    new_beam = beam + [token] #expand\n",
    "                    new_prob = prob + log_prob.item()  #sum psoibility\n",
    "                    candidates.append((new_beam, new_prob))\n",
    "            \n",
    "            #get top k beams\n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "    best = max(beams, key=lambda x: x[1])[0]\n",
    "    outputs = torch.tensor(best).unsqueeze(0) #shape [1, n]\n",
    "\n",
    "    # =========================\n",
    "    # Your code ends here\n",
    "    # =========================\n",
    "\n",
    "    return outputs\n",
    "\n",
    "generated_ids = beam_search_generation(input_ids, model, beam_size=4)[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3: Sampling\n",
    "\n",
    "Sampling method randomly selects the next word based on the probability distribution of words, instead of always choosing the word with the highest probability. This increases the diversity of generated outputs but lacks stability in results.\n",
    "\n",
    "It can be represented by the formula:\n",
    "\n",
    "$ w_t \\sim \\text{Top-}k\\left(P(w_t | w_{<t})\\right) $\n",
    "\n",
    "This formula introduces a top-k constraint to the sampling process, where instead of sampling from the entire probability distribution $ P(w_t | w_{<t}) $, we first select the top $ k $ most likely words according to this distribution, and then sample from this restricted set of words. This helps in controlling the diversity of the generated outputs while still allowing for some randomness in the sampling process.\n",
    "\n",
    "In this context, $ \\text{Top-}k\\left(P(w_t | w_{<t})\\right) $ represents the selection of the top $ k $ words with the highest probabilities from the distribution $ P(w_t | w_{<t}) $, and $ w_t $ is sampled from this restricted set of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this assignment, we will delve into different generation methods of GPT. To be specific, you will\n",
      "generated_text: In this assignment, we will delve into different generation methods of GPT. To be specific, you will be using a set of different GPT generation methods to generate GPTs.\n",
      "\n",
      "GPT Generation Methods:\n",
      "\n",
      "Generating GPTs\n",
      "\n",
      "The GPT generation method is called ‚ÄòGPT Generation Method‚Äô in Java. It consists of a set of different GPT generation methods. The method\n"
     ]
    }
   ],
   "source": [
    "def sampling_generation(input_ids, model, topk) -> str:\n",
    "    outputs = input_ids\n",
    "    \n",
    "    # =========================\n",
    "    # Your code starts here (3 points)\n",
    "    # outputs should be a torch Tensor of shape [1, n]\n",
    "    # where n is the length of the generated text\n",
    "    # and the elements are the token ids of the generated text\n",
    "    # \n",
    "    # You should not use any external library like transformers's .generate().\n",
    "    # Only torch is allowed.\n",
    "    # =========================\n",
    "    \n",
    "    max_length = 64 #max length as in announcement\n",
    "    with torch.no_grad():  \n",
    "        for _ in range(max_length):\n",
    "            logits = model(outputs).logits[:, -1, :]\n",
    "            \n",
    "            #get top k tokens\n",
    "            top_logit, top_id = torch.topk(logits, topk)\n",
    "            topk_probs = torch.softmax(top_logit, dim=-1) \n",
    "\n",
    "            #random sample one\n",
    "            next_id = torch.multinomial(topk_probs, num_samples=1)\n",
    "            next_id = top_id.gather(-1, next_id)  \n",
    "     \n",
    "            outputs = torch.cat([outputs, next_id], dim=-1)\n",
    "            \n",
    "            if next_id == tokenizer.eos_token_id: #eos\n",
    "                break\n",
    "    \n",
    "    # =========================\n",
    "    # Your code ends here\n",
    "    # =========================\n",
    "\n",
    "    return outputs\n",
    "\n",
    "generated_ids = sampling_generation(input_ids, model, topk=5)[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
