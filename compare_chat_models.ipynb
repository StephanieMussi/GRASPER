{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install openai\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install models and put the .gguf files in models/ folder\n",
    "## Llama2:\n",
    "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_K_M.gguf\n",
    "## Mistral\n",
    "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
    "## Gemma\n",
    "https://huggingface.co/mlabonne/gemma-7b-it-GGUF/blob/main/gemma-7b-it.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load summary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"gpt_summary.txt\", \"r\")\n",
    "final_summary = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 1\n",
      "Title: Introduction\n",
      "Content: \n",
      "- Introduction of the problem of Generalized Category Discovery (GCD) in fine-grained datasets.\n",
      "- Description of the challenges faced in fine-grained classification tasks due to high class similarities and intra-class variances.\n",
      "\n",
      "Slide 2\n",
      "Title: Related Works\n",
      "Content: \n",
      "- Overview of Novel Category Discovery (NCD) and its relevance to XCon's objectives.\n",
      "- Discussion on previous approaches utilizing transfer learning, self-supervision, and contrastive learning in categorizing unseen classes.\n",
      "\n",
      "Slide 3\n",
      "Title: Methodology\n",
      "Content: \n",
      "- Explanation of Expert-Contrastive Learning (XCon) approach.\n",
      "- Description of partitioning the dataset into expert sub-datasets using k-means clustering.\n",
      "- Details on supervised and unsupervised contrastive learning across full and sub-datasets.\n",
      "\n",
      "Slide 4\n",
      "Title: Experiments and Results\n",
      "Content: \n",
      "- Evaluation of XCon on CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, and Oxford-IIIT Pet datasets.\n",
      "- Showcase of state-of-the-art performance in fine-grained category discovery benchmarks.\n",
      "- Implementation details using ViT-B-16 model and DINO-pretrained parameters.\n",
      "\n",
      "Slide 5\n",
      "Title: Ablation Studies\n",
      "Content: \n",
      "- Overview of ablation studies conducted to validate components of XCon.\n",
      "- Discussion on the impact of weight of fine-grained loss and the number of sub-datasets.\n",
      "- Insights into optimal settings for different datasets based on experimental results.\n",
      "\n",
      "Slide 6\n",
      "Title: Conclusion\n",
      "Content: \n",
      "- Summary of XCon's significance in fine-grained category discovery.\n",
      "- Mention of the method's success across various benchmarks and its contribution to unsupervised and semi-supervised learning in fine-grained classification.\n",
      "- Encouragement for further exploration and adaptation of XCon within the research community.\n",
      "CPU times: user 177 ms, sys: 33.6 ms, total: 210 ms\n",
      "Wall time: 6.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = 'YOUR-API-KEY-HERE')\n",
    "\n",
    "completion_gpt3 = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": final_summary},\n",
    "    {\"role\": \"user\", \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of \\\"Slide 1\\nTitle: (title)\\nContent: (content)\\\". Do not add other information.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "message_gpt3 = completion_gpt3.choices[0].message.content\n",
    "print(message_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_gpt3.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "\n",
      "llama_print_timings:        load time =   23564.41 ms\n",
      "llama_print_timings:      sample time =     175.76 ms /   882 runs   (    0.20 ms per token,  5018.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   53802.71 ms /  1136 tokens (   47.36 ms per token,    21.11 tokens per second)\n",
      "llama_print_timings:        eval time =  119623.43 ms /   881 runs   (  135.78 ms per token,     7.36 tokens per second)\n",
      "llama_print_timings:       total time =  175706.13 ms /  2017 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here is the presentation slides based on the provided text:\n",
      "Slide 1: Introduction\n",
      "Title: Learning with Experts for Fine-grained Category Discovery\n",
      "Content: The paper \"XCon: Learning with Experts for Fine-grained Category Discovery\" by Yixin Fei et al. introduces a novel approach to address the problem of Generalized Category Discovery (GCD) in fine-grained datasets. GCD aims to cluster unlabeled images by leveraging information from a set of seen (labeled) classes, which is challenging due to large inter-class similarities and intra-class variances. XCon enhances the model's ability to discern fine-grained discriminative features by partitioning the dataset into sub-datasets using k-means clustering and applying contrastive learning within these partitions.\n",
      "Slide 2: Related Works\n",
      "Title: Novel Category Discovery vs Fine-grained Category Discovery\n",
      "Content: Earlier works in Novel Category Discovery (NCD) have utilized transfer learning and self-supervision techniques to categorize unseen classes based on knowledge transferred from seen classes. However, these methods are limited in their effectiveness in fine-grained classification tasks where class distinctions are more nuanced. XCon addresses this challenge by leveraging expert sub-datasets and a combination of contrastive learning approaches.\n",
      "Slide 3: Methodology\n",
      "Title: Partitioning the Dataset using k-means Clustering\n",
      "Content: XCon partitions the dataset into several sub-datasets (referred to as expert datasets) using k-means clustering on self-supervised representations. This partitioning is based on the insight that class-irrelevant cues can lead to misleading clustering, thus, by creating more homogenized sub-datasets, the model is compelled to learn finer distinctions between classes.\n",
      "Slide 4: Learning Process\n",
      "Title: Supervised and Unsupervised Contrastive Learning\n",
      "Content: The learning process involves both supervised and unsupervised contrastive learning across the full dataset and within the sub-datasets. The losses from these learning processes are combined to form the model's final optimization objective. This dual approach allows XCon to learn from both coarse-grained (overall dataset) and fine-grained (sub-dataset) features.\n",
      "Slide 5: Experiments and Results\n",
      "Title: State-of-the-Art Performance on Fine-grained Category Discovery Benchmarks\n",
      "Content: The effectiveness of XCon was evaluated on various datasets, including CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, and Oxford-IIIT Pet. These datasets encompass a wide range of classification challenges, from generic to fine-grained categories. XCon achieved state-of-the-art performance on several fine-grained category discovery benchmarks, demonstrating its capability to enhance classification performance significantly in challenging datasets.\n",
      "Slide 6: Ablation Studies\n",
      "Title: Validating the Effectiveness of XCon Components\n",
      "Content: Ablation studies were conducted to validate the effectiveness of various components of XCon, including the impact of the weight of fine-grained loss and the number of sub-datasets. These studies confirmed the robustness of the XCon approach across different configurations and provided insights into optimal settings for different datasets.\n",
      "Slide 7: Conclusion\n",
      "Title: A Novel Approach to Fine-grained Category Discovery\n",
      "Content: XCon introduces a novel method for fine-grained category discovery by leveraging expert sub-datasets and a combination of contrastive learning approaches. Its success across various benchmarks signifies a significant advancement in the field, offering a new direction for research in unsupervised and semi-supervised learning within fine-grained classification scenarios. The method's codebase has been made publicly available, encouraging further exploration and adaptation of this approach within the research community.\n",
      "CPU times: user 22min 29s, sys: 37.6 s, total: 23min 6s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "      chat_format=\"llama-2\", \n",
    "      n_ctx=4096\n",
    ")\n",
    "completion_llama2 = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": final_summary},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of \\\"Slide 1\\nTitle: (title)\\nContent: (content)\\\". Do not add other information. \"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "message_llama2 = completion_llama2['choices'][0]['message']['content']\n",
    "print(message_llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_llama2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "\n",
      "llama_print_timings:        load time =   25017.68 ms\n",
      "llama_print_timings:      sample time =     109.81 ms /   564 runs   (    0.19 ms per token,  5136.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55935.47 ms /  1111 tokens (   50.35 ms per token,    19.86 tokens per second)\n",
      "llama_print_timings:        eval time =   72816.60 ms /   563 runs   (  129.34 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:       total time =  130222.66 ms /  1674 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Slide 1\n",
      "Title: Introduction to Generalized Category Discovery (GCD) and XCon\n",
      "Content:\n",
      "- GCD: a practical approach for categorizing unlabeled instances in the absence of knowledge about all classes\n",
      "- XCon: a method for fine-grained GCD using expert datasets and contrastive learning\n",
      "\n",
      "Slide 2\n",
      "Title: Motivation: Challenges in Fine-Grained Category Discovery\n",
      "Content:\n",
      "- Large inter-class similarities and intra-class variances in fine-grained datasets\n",
      "- Traditional methods often struggle with irrelevant cues, leading to suboptimal performance\n",
      "\n",
      "Slide 3\n",
      "Title: Related Work: Novel Category Discovery (NCD) and Contrastive Learning\n",
      "Content:\n",
      "- NCD: categorizing unseen classes based on knowledge from seen classes\n",
      "- Transfer learning and self-supervision techniques used in NCD\n",
      "- Limitations of contrastive learning for fine-grained classification\n",
      "\n",
      "Slide 4\n",
      "Title: Methodology: XCon's Approach to Fine-Grained Category Discovery\n",
      "Content:\n",
      "- Partitioning dataset into expert datasets using k-means clustering on self-supervised representations\n",
      "- Learning from both coarse-grained and fine-grained features through supervised and unsupervised contrastive learning\n",
      "\n",
      "Slide 5\n",
      "Title: Experiments and Results: State-of-the-Art Performance on Various Datasets\n",
      "Content:\n",
      "- CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, Oxford-IIIT Pet\n",
      "- Achieved state-of-the-art performance on several fine-grained category discovery benchmarks\n",
      "\n",
      "Slide 6\n",
      "Title: Implementation Details: Model Architecture and Training\n",
      "Content:\n",
      "- ViT-B-16 model initialized with DINO-pretrained parameters\n",
      "- Fine-tuning final transformer block\n",
      "- Combination of supervised and unsupervised contrastive losses\n",
      "\n",
      "Slide 7\n",
      "Title: Ablation Studies: Validating XCon's Effectiveness\n",
      "Content:\n",
      "- Impact of fine-grained loss weight and number of sub-datasets on performance\n",
      "- Robustness of XCon across different configurations\n",
      "\n",
      "Slide 8\n",
      "Title: Conclusion: Significant Advancement in Fine-Grained Category Discovery\n",
      "Content:\n",
      "- XCon's success on various benchmarks marks a significant advancement in the field\n",
      "- Encourages further exploration and adaptation within the research community.\n",
      "CPU times: user 16min 37s, sys: 30.7 s, total: 17min 8s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "mistral = Llama(\n",
    "      model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "      chat_format=\"llama-2\", \n",
    "      n_ctx=4096\n",
    ")\n",
    "completion_mistral = mistral.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": final_summary},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of \\\"Slide 1\\nTitle: (title)\\nContent: (content)\\\". Do not add other information. \"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "message_mistral = completion_mistral['choices'][0]['message']['content']\n",
    "print(message_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_mistral.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ppt files for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 159 ms, sys: 4.02 ms, total: 163 ms\n",
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pptx import Presentation\n",
    "import copy\n",
    "\n",
    "template_path = './template.pptx'\n",
    "\n",
    "def add_title_slide(title, prs):\n",
    "    first_slide = prs.slides[0]\n",
    "    title_shape = first_slide.shapes[0]\n",
    "    if title_shape.has_text_frame:\n",
    "        p = title_shape.text_frame.paragraphs[0]  \n",
    "        run = p.runs[0] if p.runs else p.add_run()  \n",
    "        run.text = title \n",
    "        \n",
    "def add_content_slide(title, content, prs):\n",
    "    first_slide = prs.slides[-1]\n",
    "    title_shape = first_slide.shapes[0]\n",
    "    if title_shape.has_text_frame:\n",
    "        p = title_shape.text_frame.paragraphs[0]  \n",
    "        run = p.runs[0] if p.runs else p.add_run()  \n",
    "        run.text = title \n",
    "    content_shape = first_slide.shapes[1]\n",
    "    if content_shape.has_text_frame:\n",
    "        p = content_shape.text_frame.paragraphs[0]  \n",
    "        run = p.runs[0] if p.runs else p.add_run()  \n",
    "        run.text = content \n",
    "\n",
    "def duplicate_slide(presentation, slide_index):\n",
    "    slide = presentation.slides[slide_index]\n",
    "    layout = slide.slide_layout\n",
    "    duplicated_slide = presentation.slides.add_slide(layout)\n",
    "    \n",
    "    for shape in slide.shapes:\n",
    "        el = shape.element\n",
    "        new_el = copy.deepcopy(el)\n",
    "        duplicated_slide.shapes._spTree.insert_element_before(new_el, 'p:extLst')\n",
    "\n",
    "    return presentation\n",
    "\n",
    "\n",
    "def generate_slides(message):\n",
    "    slides = message.strip().split(\"Slide \")[1:]\n",
    "    prs = Presentation(template_path)\n",
    "\n",
    "    add_title_slide('XCon Learning with Experts for Fine-grained Category Discovery',prs)\n",
    "\n",
    "    for i, slide in enumerate(slides):\n",
    "        parts = slide.split(\"\\nTitle:\")\n",
    "        slide_number = parts[0].strip()  \n",
    "        title_content = parts[1].split(\"\\nContent:\")\n",
    "        title = title_content[0].strip()\n",
    "        content = title_content[1].strip()\n",
    "        if i > 0:\n",
    "            prs = duplicate_slide(prs, -1)\n",
    "        add_content_slide(title, content, prs)\n",
    "        \n",
    "    return prs\n",
    "\n",
    "prs_gpt3 = generate_slides(message_gpt3)\n",
    "prs_gpt3.save('./presentation_generated/presentation_gpt3.pptx')\n",
    "\n",
    "prs_llama2 = generate_slides(message_llama2)\n",
    "prs_llama2.save('./presentation_generated/presentation_llama2.pptx')\n",
    "\n",
    "prs_mistral = generate_slides(message_mistral)\n",
    "prs_mistral.save('./presentation_generated/presentation_mistral.pptx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gpt-4 converted slides content as standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"gpt_slides_content.txt\", \"r\")\n",
    "slides_content_gpt4 = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
