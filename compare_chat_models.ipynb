{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install openai\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load summary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"gpt_summary.txt\", \"r\")\n",
    "final_summary = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gpt-4 converted slides content as standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"gpt_slides_content.txt\", \"r\")\n",
    "slides_content_gpt4 = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 1\n",
      "Title: XCon: Learning with Experts for Fine-grained Category Discovery\n",
      "Content: Introducing a novel approach for Generalized Category Discovery (GCD) in fine-grained datasets.\n",
      "\n",
      "Slide 2\n",
      "Title: Introduction\n",
      "Content: Deep learning advancements in computer vision, challenges in fine-grained recognition, and the significance of Generalized Category Discovery (GCD).\n",
      "\n",
      "Slide 3\n",
      "Title: Related Works\n",
      "Content: Overview of Novel Category Discovery (NCD), transfer learning, self-supervision, and contrastive learning in the context of GCD tasks.\n",
      "\n",
      "Slide 4\n",
      "Title: Methodology\n",
      "Content: Description of Expert-Contrastive Learning (XCon) approach, including dataset partitioning with k-means clustering, supervised and unsupervised contrastive learning, and optimization objective.\n",
      "\n",
      "Slide 5\n",
      "Title: Experiments and Results\n",
      "Content: Evaluation of XCon on CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, and Oxford-IIIT Pet datasets, showcasing state-of-the-art performance in fine-grained category discovery benchmarks.\n",
      "\n",
      "Slide 6\n",
      "Title: Implementation Details\n",
      "Content: Implementation specifics like ViT-B-16 model initialization with DINO-pretrained parameters, fine-tuning, supervised and unsupervised contrastive losses, and post-training steps.\n",
      "\n",
      "Slide 7\n",
      "Title: Ablation Studies\n",
      "Content: Validation of XCon components through ablation studies, including the impact of fine-grained loss weight and number of sub-datasets, offering insights into optimal configurations.\n",
      "\n",
      "Slide 8\n",
      "Title: Conclusion\n",
      "Content: Summary of XCon's advancements in fine-grained category discovery, its potential for unsupervised and semi-supervised learning, and encouragement for further exploration with the publicly available codebase.\n",
      "CPU times: user 199 ms, sys: 15.5 ms, total: 214 ms\n",
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = 'YOUR-API-KEY-HERE')\n",
    "\n",
    "completion_gpt3 = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": final_summary},\n",
    "    {\"role\": \"user\", \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of Slide 1 Title: (title)\\nContent: (content). Do not add other information.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "message_gpt3 = completion_gpt3.choices[0].message.content\n",
    "print(message_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_gpt3.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "\n",
      "llama_print_timings:        load time =   24707.03 ms\n",
      "llama_print_timings:      sample time =     136.64 ms /   656 runs   (    0.21 ms per token,  4801.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55677.76 ms /  1133 tokens (   49.14 ms per token,    20.35 tokens per second)\n",
      "llama_print_timings:        eval time =   94090.12 ms /   655 runs   (  143.65 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:       total time =  151497.98 ms /  1788 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here is the presentation slides based on the provided text:\n",
      "Slide 1: Introduction\n",
      "Title: Generalized Category Discovery and XCon\n",
      "Content:\n",
      "* Introduces a novel approach to address the problem of Generalized Category Discovery (GCD) in computer vision tasks.\n",
      "Slide 2: Related Works\n",
      "Title: Overview of Novel Category Discovery and Contrastive Learning\n",
      "Content:\n",
      "* Earlier works in NCD have utilized transfer learning and self-supervision techniques to categorize unseen classes based on knowledge transferred from seen classes.\n",
      "Slide 3: Methodology\n",
      "Title: Partitioning the Dataset using k-means Clustering\n",
      "Content:\n",
      "* XCon partitions the dataset into several sub-datasets (referred to as expert datasets) using k-means clustering on self-supervised representations.\n",
      "Slide 4: Learning Process\n",
      "Title: Dual Approach to Learning - Supervised and Unsupervised Contrastive Learning\n",
      "Content:\n",
      "* XCon learns from both coarse-grained (overall dataset) and fine-grained (sub-dataset) features through a dual approach that combines supervised and unsupervised contrastive learning across the full dataset and within the sub-datasets.\n",
      "Slide 5: Experiments and Results\n",
      "Title: XCon Achieves State-of-the-Art Performance on Fine-Grained Category Discovery Benchmarks\n",
      "Content:\n",
      "* The effectiveness of XCon was evaluated on various datasets, including CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, and Oxford-IIIT Pet. These datasets encompass a wide range of classification challenges, from generic to fine-grained categories. XCon achieved state-of-the-art performance on several fine-grained category discovery benchmarks, demonstrating its capability to enhance classification performance significantly in challenging datasets.\n",
      "Slide 6: Ablation Studies\n",
      "Title: Validating the Effectiveness of XCon Components\n",
      "Content:\n",
      "* Ablation studies were conducted to validate the effectiveness of various components of XCon, including the impact of the weight of fine-grained loss and the number of sub-datasets. These studies confirmed the robustness of the XCon approach across different configurations and provided insights into optimal settings for different datasets.\n",
      "Slide 7: Conclusion\n",
      "Title: XCon - A Novel Approach to Fine-Grained Category Discovery\n",
      "Content:\n",
      "* Introduces a novel method for fine-grained category discovery by leveraging expert sub-datasets and a combination of contrastive learning approaches. Its success across various benchmarks signifies a significant advancement in the field, offering a new direction for research in unsupervised and semi-supervised learning within fine-grained classification scenarios. The method's codebase has been made publicly available, encouraging further exploration and adaptation of this approach within the research community.\n",
      "CPU times: user 19min 24s, sys: 31.4 s, total: 19min 55s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "      chat_format=\"llama-2\", \n",
    "      n_ctx=4096\n",
    ")\n",
    "completion_llama2 = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": final_summary},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of Slide 1 Title: (title)\\nContent: (content). Do not add other information. \"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "message_llama2 = completion_llama2['choices'][0]['message']['content']\n",
    "print(message_llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_llama2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "\n",
      "llama_print_timings:        load time =   25807.20 ms\n",
      "llama_print_timings:      sample time =     117.07 ms /   568 runs   (    0.21 ms per token,  4851.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56825.95 ms /  1108 tokens (   51.29 ms per token,    19.50 tokens per second)\n",
      "llama_print_timings:        eval time =   75969.83 ms /   567 runs   (  133.99 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:       total time =  134244.54 ms /  1675 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Slide 1: Introduction to Generalized Category Discovery (GCD)\n",
      "Title: GCD: Leveraging Labeled Data for Unlabeled Instance Categorization\n",
      "Content: With the advance of deep learning, progress in computer vision tasks relies heavily on large-scale annotated data. However, collecting and annotating such datasets for fine-grained recognition tasks is challenging due to expertise required and new classes emerging continuously. GCD proposes a solution by utilizing labeled datasets to categorize unlabeled instances based on implicit criteria set by the labeled data.\n",
      "\n",
      "Slide 2: Motivation and Related Works\n",
      "Title: Motivation: Challenges in Fine-Grained Category Discovery\n",
      "Content: Traditional methods struggle with large inter-class similarities and intra-class variances, leading to suboptimal performance due to irrelevant cues. Novel Category Discovery (NCD) has been proposed as an alternative, but its effectiveness in fine-grained classification is limited.\n",
      "\n",
      "Slide 3: XCon Methodology\n",
      "Title: XCon: Expert-Contrastive Learning for Fine-Grained GCD\n",
      "Content: Partitioning dataset into sub-datasets using k-means clustering on self-supervised representations, followed by supervised and unsupervised contrastive learning. Dual approach allows learning from coarse-grained and fine-grained features.\n",
      "\n",
      "Slide 4: XCon Experiments and Results\n",
      "Title: Evaluation of XCon: State-of-the-Art Performance on Various Datasets\n",
      "Content: XCon achieved superior results on CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, and Oxford-IIIT Pet. Demonstrating significant enhancement in challenging datasets.\n",
      "\n",
      "Slide 5: XCon Implementation Details\n",
      "Title: Implementation of XCon: ViT-B-16 Model with DINO Pretrained Parameters\n",
      "Content: Fine-tuning final transformer block, using supervised and unsupervised contrastive losses, discarding projection heads post-training.\n",
      "\n",
      "Slide 6: Ablation Studies in XCon\n",
      "Title: Validating Components of XCon: Impact on Performance\n",
      "Content: Studies confirmed robustness across different configurations and provided insights into optimal settings for various datasets.\n",
      "\n",
      "Slide 7: Conclusion\n",
      "Title: Advancements in Fine-Grained Category Discovery with XCon\n",
      "Content: XCon offers a significant advancement in the field, encouraging further exploration and adaptation within the research community. Codebase is publicly available.\n",
      "CPU times: user 17min 9s, sys: 31.1 s, total: 17min 40s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "mistral = Llama(\n",
    "      model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "      chat_format=\"llama-2\", \n",
    "      n_ctx=4096\n",
    ")\n",
    "completion_mistral = mistral.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": final_summary},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of Slide 1 Title: (title)\\nContent: (content). Do not add other information. \"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "message_mistral = completion_mistral['choices'][0]['message']['content']\n",
    "print(message_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_mistral.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "\n",
      "llama_print_timings:        load time =   25689.05 ms\n",
      "llama_print_timings:      sample time =     125.40 ms /   599 runs   (    0.21 ms per token,  4776.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56957.61 ms /  1108 tokens (   51.41 ms per token,    19.45 tokens per second)\n",
      "llama_print_timings:        eval time =   82834.97 ms /   598 runs   (  138.52 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:       total time =  141360.77 ms /  1706 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Slide 1: Introduction to Generalized Category Discovery (GCD)\n",
      "Title: GCD: Leveraging Labeled Data for Unlabeled Fine-Grained Recognition\n",
      "Content: With deep learning progress, computer vision tasks have made significant strides. However, collecting and annotating large datasets for fine-grained recognition remains a challenge. GCD is proposed as an alternative to semi-supervised learning, focusing on using labeled data to categorize unlabeled instances based on implicit criteria.\n",
      "\n",
      "Slide 2: Motivation and Related Work\n",
      "Title: Motivation: Challenges in Fine-Grained Category Discovery\n",
      "Content: Traditional methods struggle with large inter-class similarities and intra-class variances, leading to suboptimal performance. Novel Category Discovery (NCD) and Contrastive Learning are related approaches, but their effectiveness in fine-grained classification is limited.\n",
      "\n",
      "Slide 3: XCon Methodology\n",
      "Title: XCon: Expert-Contrastive Learning for Fine-Grained GCD\n",
      "Content: XCon partitions the dataset into expert datasets using k-means clustering on self-supervised representations, allowing the model to learn finer distinctions between classes. Dual learning with supervised and unsupervised contrastive losses is employed.\n",
      "\n",
      "Slide 4: Experiments and Results\n",
      "Title: XCon: State-of-the-Art Performance in Fine-Grained Category Discovery\n",
      "Content: XCon achieved top results on various datasets, including CIFAR-10/100, ImageNet-100, CUB-200, Stanford Cars, FGVC-Aircraft, and Oxford-IIIT Pet. The method's success signifies a significant advancement in the field.\n",
      "\n",
      "Slide 5: Implementation Details\n",
      "Title: XCon: Implementation and Training\n",
      "Content: XCon uses a ViT-B-16 model initialized with DINO-pretrained parameters, fine-tunes the final transformer block, and employs supervised and unsupervised contrastive losses. The projections heads are discarded post-training.\n",
      "\n",
      "Slide 6: Ablation Studies\n",
      "Title: XCon: Validating Components' Impact\n",
      "Content: Ablation studies confirmed the robustness of XCon across different configurations, providing insights into optimal settings for various datasets.\n",
      "\n",
      "Slide 7: Conclusion and Future Work\n",
      "Title: XCon: Significant Advancement in Fine-Grained Category Discovery\n",
      "Content: XCon's success offers a new direction for research in unsupervised and semi-supervised learning within fine-grained classification scenarios. The method's codebase is publicly available, encouraging further exploration and adaptation.\n",
      "CPU times: user 18min 5s, sys: 30.9 s, total: 18min 35s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "gemma = Llama(\n",
    "      model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "      chat_format=\"llama-2\", \n",
    "      n_ctx=4096\n",
    ")\n",
    "completion_gemma = gemma.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": final_summary},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Please convert the text into a presentation slides. Give the title and actual detailed content for each slide, in the format of Slide 1 Title: (title)\\nContent: (content). Do not add other information. \"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "message_gemma = completion_gemma['choices'][0]['message']['content']\n",
    "print(message_gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./presentation_generated/slides_content_gemma.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(message_gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ppt files for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 ms, sys: 0 ns, total: 103 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pptx import Presentation\n",
    "\n",
    "def add_slide(title, content):\n",
    "        slide_layout = prs.slide_layouts[1]  \n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        title_placeholder = slide.shapes.title\n",
    "        body_placeholder = slide.placeholders[1]\n",
    "        title_placeholder.text = title\n",
    "        body_placeholder.text = content\n",
    "        \n",
    "def generate_slides(message):\n",
    "    slides = message.strip().split(\"Slide \")[1:]\n",
    "    prs = Presentation()\n",
    "\n",
    "    for slide in slides:\n",
    "        parts = slide.split(\"\\nTitle:\")\n",
    "        slide_number = parts[0].strip()  \n",
    "        title_content = parts[1].split(\"\\nContent:\")\n",
    "        title = title_content[0].strip()\n",
    "        content = title_content[1].strip()\n",
    "        add_slide(title, content)\n",
    "        \n",
    "    return prs\n",
    "\n",
    "prs_gpt3 = generate_slides(message_gpt3)\n",
    "prs_gpt3.save('./presentation_generated/presentation_gpt3.pptx')\n",
    "\n",
    "prs_llama2 = generate_slides(message_llama2)\n",
    "prs_llama2.save('./presentation_generated/presentation_llama2.pptx')\n",
    "\n",
    "prs_mistral = generate_slides(message_mistral)\n",
    "prs_mistral.save('./presentation_generated/presentation_mistral.pptx')\n",
    "\n",
    "prs_gemma = generate_slides(message_gemma)\n",
    "prs_gemma.save('./presentation_generated/presentation_gemma.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
