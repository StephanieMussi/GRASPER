{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read extracted text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"extracted_text.txt\", \"r\", encoding=\"utf-8\")\n",
    "long_text = file.read()\n",
    "print(long_text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split text into trunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "def chunk_text_into_tokens(text, tokenizer, max_tokens=500):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    count = len(token_chunks)\n",
    "    return token_chunks, count\n",
    "\n",
    "max_text_length = 4096\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(token_chunks, max_sum_length, min_sum_length, tokenizer, model):\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in token_chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        inputs = tokenizer(chunk_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "        summary_ids = model.generate(**inputs, max_length=max_sum_length, min_length=min_sum_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def concatenate_summaries(summaries):\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_bart)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_bart = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_bart, model_bart)\n",
    "\n",
    "final_summary_bart = concatenate_summaries(chunk_summaries_bart)\n",
    "\n",
    "tokens_bart = tokenizer_bart.tokenize(final_summary_bart)\n",
    "number_of_tokens_bart = len(tokens_bart)\n",
    "print(\"number_of_tokens: \", number_of_tokens_bart)\n",
    "\n",
    "print(final_summary_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_bart.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_bart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks_T5(chunks, max_sum_length, min_sum_length, tokenizer, model):\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        inputs = tokenizer(\"summarize: \"+chunk_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "        summary_ids = model.generate(**inputs, max_length=max_sum_length, min_length=min_sum_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer_T5= T5Tokenizer.from_pretrained(\"t5-base\", legacy=False) \n",
    "model_T5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_T5)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_T5 = summarize_chunks_T5(text_chunks, max_sum_length, min_sum_length, tokenizer_T5, model_T5)\n",
    "\n",
    "final_summary_T5 = concatenate_summaries(chunk_summaries_T5)\n",
    "\n",
    "tokens_T5 = tokenizer_T5.tokenize(final_summary_T5)\n",
    "number_of_tokens_T5 = len(tokens_T5)\n",
    "print(\"number_of_tokens: \", number_of_tokens_T5)\n",
    "\n",
    "print(final_summary_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_T5.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "tokenizer_LED = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "model_LED = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_LED)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_LED = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_LED, model_LED)\n",
    "\n",
    "final_summary_LED = concatenate_summaries(chunk_summaries_LED)\n",
    "\n",
    "tokens_LED = tokenizer_LED.tokenize(final_summary_LED)\n",
    "number_of_tokens_LED = len(tokens_LED)\n",
    "print(\"number_of_tokens: \", number_of_tokens_LED)\n",
    "\n",
    "print(final_summary_LED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_LED.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_LED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer_Pegasus = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model_Pegasus = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_Pegasus)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_Pegasus = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_Pegasus, model_Pegasus)\n",
    "\n",
    "final_summary_Pegasus = concatenate_summaries(chunk_summaries_Pegasus)\n",
    "\n",
    "tokens_Pegasus = tokenizer_Pegasus.tokenize(final_summary_Pegasus)\n",
    "number_of_tokens_Pegasus = len(tokens_Pegasus)\n",
    "print(\"number_of_tokens: \", number_of_tokens_Pegasus)\n",
    "\n",
    "print(final_summary_Pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_Pegasus.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_Pegasus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: BigBirdPegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n",
    "\n",
    "tokenizer_BigBird = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "model_BigBird = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_BigBird)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_BigBird = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_BigBird, model_BigBird)\n",
    "\n",
    "final_summary_BigBird = concatenate_summaries(chunk_summaries_BigBird)\n",
    "\n",
    "tokens_BigBird = tokenizer_BigBird.tokenize(final_summary_BigBird)\n",
    "number_of_tokens_BigBird = len(tokens_BigBird)\n",
    "print(\"number_of_tokens: \", number_of_tokens_BigBird)\n",
    "\n",
    "print(final_summary_BigBird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_BigBird.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_BigBird)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: ProphetNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer, ProphetNetForConditionalGeneration\n",
    "\n",
    "tokenizer_prophetNet = AutoTokenizer.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "model_prophetNet = ProphetNetForConditionalGeneration.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_prophetNet)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_prophetNet = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_prophetNet, model_prophetNet)\n",
    "\n",
    "final_summary_prophetNet = concatenate_summaries(chunk_summaries_prophetNet)\n",
    "\n",
    "tokens_prophetNet = tokenizer_blenderBot.tokenize(final_summary_prophetNet)\n",
    "number_of_tokens_prophetNet = len(tokens_prophetNet)\n",
    "print(\"number_of_tokens: \", number_of_tokens_prophetNet)\n",
    "\n",
    "print(final_summary_prophetNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_prophetNet.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_prophetNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: LlaMa2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "login(token = 'YOUR-HUGGINGFACE-TOKEN')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "    \n",
    "tokenizer_llama2 = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model_llama2 = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_llama2)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_llama2 = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_llama2, model_llama2)\n",
    "\n",
    "final_summary_llama2 = concatenate_summaries(chunk_summaries_llama2)\n",
    "\n",
    "tokens_llama2 = tokenizer_llama2.tokenize(final_summary_llama2)\n",
    "number_of_tokens_llama2 = len(tokens_llama2)\n",
    "print(\"number_of_tokens: \", number_of_tokens_llama2)\n",
    "\n",
    "print(final_summary_llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Mistral"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_mistral)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_mistral = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_mistral, model_mistral)\n",
    "\n",
    "final_summary_mistral = concatenate_summaries(chunk_summaries_mistral)\n",
    "\n",
    "tokens_mistral = tokenizer_mistral.tokenize(final_summary_mistral)\n",
    "number_of_tokens_mistral = len(tokens_mistral)\n",
    "print(\"number_of_tokens: \", number_of_tokens_mistral)\n",
    "\n",
    "print(final_summary_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Gemma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, GemmaForCausalLM\n",
    "\n",
    "model_gemma = GemmaForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_gemma)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_gemma = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_gemma, model_gemma)\n",
    "\n",
    "final_summary_gemma = concatenate_summaries(chunk_summaries_gemma)\n",
    "\n",
    "tokens_gemma = tokenizer_gemma.tokenize(final_summary_gemma)\n",
    "number_of_tokens_gemma = len(tokens_gemma)\n",
    "print(\"number_of_tokens: \", number_of_tokens_gemma)\n",
    "\n",
    "print(final_summary_gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read GPT4 generated summary as standard for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"gpt_summary.txt\", \"r\", encoding=\"utf-8\")\n",
    "gpt_summary = file.read()\n",
    "print(gpt_summary)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria: ROUGE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Initialize the rouge evaluator\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Your data: a list of reference summaries and a list of generated summaries\n",
    "references = gpt_summary\n",
    "predictions = [\"Your model generated summary text here.\"]\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"ROUGE scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria: BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the bleu evaluator\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria: METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the meteor evaluator\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"METEOR score:\", meteor_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Length Analysis\n",
    "Objective: Evaluate if the generated summaries have an appropriate length relative to the original documents.\n",
    "Method:\n",
    "Calculate the ratio of summary length to original document length for each model. A consistent ratio across different document lengths might indicate a model's ability to adjust summary length based on content complexity.\n",
    "Set thresholds for what you consider to be too short or too long for a summary, based on preliminary tests or literature. Compare how often each model's summaries fall within these thresholds.\n",
    "\n",
    "Keyword Overlap\n",
    "Objective: Measure how well the summaries capture key information from the original documents by looking at the overlap of significant terms.\n",
    "Method:\n",
    "Extract key terms from the original documents using TF-IDF, named entity recognition, or other keyword extraction techniques.\n",
    "Calculate the proportion of these key terms that appear in each summary. A higher proportion indicates a potentially better summary.\n",
    "You can also look at the distribution of key terms to see if some summaries consistently miss certain types of information.\n",
    "\n",
    "Readability Scores\n",
    "Objective: Assess the ease of reading and understandability of the summaries.\n",
    "Method:\n",
    "Use readability formulas like Flesch Reading Ease, Flesch-Kincaid Grade Level, or Gunning Fog Index to calculate the readability of both the original texts and their summaries.\n",
    "Compare these scores to see if summaries are significantly more readable while still being informative. This comparison can be especially important for summaries intended for broader audiences.\n",
    "\n",
    "Semantic Similarity\n",
    "Objective: Evaluate the semantic preservation of the main ideas from the original text to the summary.\n",
    "Method:\n",
    "Employ semantic similarity measures, such as cosine similarity between word embeddings (e.g., Word2Vec, GloVe, or BERT embeddings) of the original text and the summary. This can help assess how well the summary captures the gist of the original content.\n",
    "Calculate the average semantic similarity across a dataset to compare different summarization models.\n",
    "\n",
    "Novelty and Redundancy Measures\n",
    "Objective: Quantify the amount of new information presented in the summaries and the degree of unnecessary repetition.\n",
    "Method:\n",
    "Novelty: Measure the proportion of n-grams or semantic concepts in the summary that are not present in the original text. While some novelty is good, too much might indicate the summary is straying from the source content.\n",
    "Redundancy: Identify repeating phrases or concepts within a summary. Summaries with high redundancy might be less effective in conveying information concisely.\n",
    "\n",
    "Content Coverage\n",
    "Objective: Determine how much of the important content in the original document is covered in the summary.\n",
    "Method:\n",
    "Split the original document into segments (e.g., sentences or paragraphs) and evaluate which segments are represented in the summary.\n",
    "Use unsupervised clustering or topic modeling to identify main themes or topics in the original document and check if these are present in the summary."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
