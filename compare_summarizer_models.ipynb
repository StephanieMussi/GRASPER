{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read extracted text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"extracted_text.txt\", \"r\", encoding=\"utf-8\")\n",
    "long_text = file.read()\n",
    "print(long_text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split text into trunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "def chunk_text_into_tokens(text, tokenizer, max_tokens=500):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    count = len(token_chunks)\n",
    "    return token_chunks, count\n",
    "\n",
    "max_text_length = 4096\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(token_chunks, max_sum_length, min_sum_length, tokenizer, model):\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in token_chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        inputs = tokenizer(chunk_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "        summary_ids = model.generate(**inputs, max_length=max_sum_length, min_length=min_sum_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def concatenate_summaries(summaries):\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_bart)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_bart = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_bart, model_bart)\n",
    "\n",
    "final_summary_bart = concatenate_summaries(chunk_summaries_bart)\n",
    "\n",
    "tokens_bart = tokenizer_bart.tokenize(final_summary_bart)\n",
    "number_of_tokens_bart = len(tokens_bart)\n",
    "print(\"number_of_tokens: \", number_of_tokens_bart)\n",
    "\n",
    "print(final_summary_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_bart.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_bart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks_T5(chunks, max_sum_length, min_sum_length, tokenizer, model):\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        inputs = tokenizer(\"summarize: \"+chunk_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "        summary_ids = model.generate(**inputs, max_length=max_sum_length, min_length=min_sum_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer_T5= T5Tokenizer.from_pretrained(\"t5-base\", legacy=False) \n",
    "model_T5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_T5)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_T5 = summarize_chunks_T5(text_chunks, max_sum_length, min_sum_length, tokenizer_T5, model_T5)\n",
    "\n",
    "final_summary_T5 = concatenate_summaries(chunk_summaries_T5)\n",
    "\n",
    "tokens_T5 = tokenizer_T5.tokenize(final_summary_T5)\n",
    "number_of_tokens_T5 = len(tokens_T5)\n",
    "print(\"number_of_tokens: \", number_of_tokens_T5)\n",
    "\n",
    "print(final_summary_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_T5.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "tokenizer_LED = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "model_LED = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_LED)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_LED = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_LED, model_LED)\n",
    "\n",
    "final_summary_LED = concatenate_summaries(chunk_summaries_LED)\n",
    "\n",
    "tokens_LED = tokenizer_LED.tokenize(final_summary_LED)\n",
    "number_of_tokens_LED = len(tokens_LED)\n",
    "print(\"number_of_tokens: \", number_of_tokens_LED)\n",
    "\n",
    "print(final_summary_LED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_LED.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_LED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer_Pegasus = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model_Pegasus = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_Pegasus)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_Pegasus = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_Pegasus, model_Pegasus)\n",
    "\n",
    "final_summary_Pegasus = concatenate_summaries(chunk_summaries_Pegasus)\n",
    "\n",
    "tokens_Pegasus = tokenizer_Pegasus.tokenize(final_summary_Pegasus)\n",
    "number_of_tokens_Pegasus = len(tokens_Pegasus)\n",
    "print(\"number_of_tokens: \", number_of_tokens_Pegasus)\n",
    "\n",
    "print(final_summary_Pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_Pegasus.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_Pegasus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: BigBirdPegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n",
    "\n",
    "tokenizer_BigBird = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "model_BigBird = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_BigBird)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_BigBird = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_BigBird, model_BigBird)\n",
    "\n",
    "final_summary_BigBird = concatenate_summaries(chunk_summaries_BigBird)\n",
    "\n",
    "tokens_BigBird = tokenizer_BigBird.tokenize(final_summary_BigBird)\n",
    "number_of_tokens_BigBird = len(tokens_BigBird)\n",
    "print(\"number_of_tokens: \", number_of_tokens_BigBird)\n",
    "\n",
    "print(final_summary_BigBird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_BigBird.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_BigBird)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: ProphetNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer, ProphetNetForConditionalGeneration\n",
    "\n",
    "tokenizer_prophetNet = AutoTokenizer.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "model_prophetNet = ProphetNetForConditionalGeneration.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_prophetNet)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_prophetNet = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_prophetNet, model_prophetNet)\n",
    "\n",
    "final_summary_prophetNet = concatenate_summaries(chunk_summaries_prophetNet)\n",
    "\n",
    "tokens_prophetNet = tokenizer_blenderBot.tokenize(final_summary_prophetNet)\n",
    "number_of_tokens_prophetNet = len(tokens_prophetNet)\n",
    "print(\"number_of_tokens: \", number_of_tokens_prophetNet)\n",
    "\n",
    "print(final_summary_prophetNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_prophetNet.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_summary_prophetNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: LlaMa2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "login(token = 'YOUR-HUGGINGFACE-TOKEN')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "    \n",
    "tokenizer_llama2 = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model_llama2 = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_llama2)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_llama2 = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_llama2, model_llama2)\n",
    "\n",
    "final_summary_llama2 = concatenate_summaries(chunk_summaries_llama2)\n",
    "\n",
    "tokens_llama2 = tokenizer_llama2.tokenize(final_summary_llama2)\n",
    "number_of_tokens_llama2 = len(tokens_llama2)\n",
    "print(\"number_of_tokens: \", number_of_tokens_llama2)\n",
    "\n",
    "print(final_summary_llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Mistral"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_mistral)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_mistral = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_mistral, model_mistral)\n",
    "\n",
    "final_summary_mistral = concatenate_summaries(chunk_summaries_mistral)\n",
    "\n",
    "tokens_mistral = tokenizer_mistral.tokenize(final_summary_mistral)\n",
    "number_of_tokens_mistral = len(tokens_mistral)\n",
    "print(\"number_of_tokens: \", number_of_tokens_mistral)\n",
    "\n",
    "print(final_summary_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Gemma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, GemmaForCausalLM\n",
    "\n",
    "model_gemma = GemmaForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "\n",
    "text_chunks, num_chunk = chunk_text_into_tokens(long_text, tokenizer_gemma)\n",
    "max_sum_length = min(500, math.floor(max_text_length / num_chunk))\n",
    "min_sum_length = math.floor(0.5 * max_sum_length)\n",
    "\n",
    "chunk_summaries_gemma = summarize_chunks(text_chunks, max_sum_length, min_sum_length, tokenizer_gemma, model_gemma)\n",
    "\n",
    "final_summary_gemma = concatenate_summaries(chunk_summaries_gemma)\n",
    "\n",
    "tokens_gemma = tokenizer_gemma.tokenize(final_summary_gemma)\n",
    "number_of_tokens_gemma = len(tokens_gemma)\n",
    "print(\"number_of_tokens: \", number_of_tokens_gemma)\n",
    "\n",
    "print(final_summary_gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: GPT-4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria: ROUGE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Initialize the rouge evaluator\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Your data: a list of reference summaries and a list of generated summaries\n",
    "references = [\"Your reference summary text here.\"]\n",
    "predictions = [\"Your model generated summary text here.\"]\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"ROUGE scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria: BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the bleu evaluator\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria: METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the meteor evaluator\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"METEOR score:\", meteor_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
